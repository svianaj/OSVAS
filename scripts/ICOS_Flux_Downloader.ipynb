{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b60736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import os\n",
    "import yaml\n",
    "from icoscp.cpb.dobj import Dobj\n",
    "from icoscp_core.icos import bootstrap\n",
    "from icoscp import cpauth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af27ef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Read YAML config\n",
    "OSVAS='/home/pn56/OSVASgh/'  # Main OSVAS path\n",
    "Station_name='Loobos'\n",
    "os.chdir(OSVAS)\n",
    "\n",
    "CONFIG_PATH = f\"config_files/Stations/{Station_name}.yml\"\n",
    "\n",
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "station_info = config[\"Station_metadata\"]\n",
    "validation_data = config[\"Validation_data\"]\n",
    "\n",
    "start_date = pd.to_datetime(validation_data[\"validation_start\"], utc=True)\n",
    "end_date = pd.to_datetime(validation_data[\"validation_end\"], utc=True)\n",
    "\n",
    "datasets = {k: v for k, v in validation_data.items() if k.startswith(\"dataset\") or k.startswith(\"dataset_\")}\n",
    "\n",
    "# Get access to ICOS\n",
    "# Authenticate using cookie (adjust if needed)\n",
    "cookie_path = \"icos_cookie.txt\"  # Or point to config\n",
    "cookie_token = open(cookie_path, \"r\").readline().strip()\n",
    "meta, data = bootstrap.fromCookieToken(cookie_token)\n",
    "cpauth.init_by(data.auth)\n",
    "\n",
    "#Test: If the authentication went well, these lines of code will not fail:\n",
    "import icoscp\n",
    "from icoscp.dobj import Dobj\n",
    "obj_flux='https://meta.icos-cp.eu/objects/dDlpnhS3XKyZjB22MUzP_nAm'\n",
    "dobj_flux=Dobj(obj_flux).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e421a435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Helper functions\n",
    "\n",
    "def fetch_flux_data(doi):\n",
    "    dobj = Dobj(doi)\n",
    "    df = dobj.data\n",
    "    return df\n",
    "\n",
    "def process_data(df, variable_map, station_info, start, end):\n",
    "    df['valid_dttm'] = pd.to_datetime(df['TIMESTAMP'], utc=True)\n",
    "    df = df[(df['valid_dttm'] >= start) & (df['valid_dttm'] <= end)].copy()\n",
    "\n",
    "    # Drop rows with missing required vars\n",
    "    source_vars = list(variable_map.values())\n",
    "    df = df.dropna(subset=source_vars)\n",
    "\n",
    "    # Add station metadata\n",
    "    df['SID'] = station_info['SID']\n",
    "    df['lat'] = station_info['lat']\n",
    "    df['lon'] = station_info['lon']\n",
    "    df['elev'] = station_info['elev']\n",
    "\n",
    "    # Rename variables\n",
    "    df = df.rename(columns={v: k for k, v in variable_map.items()})\n",
    "    selected_columns = ['valid_dttm', 'SID', 'lat', 'lon', 'elev'] + list(variable_map.keys())\n",
    "\n",
    "    return df[selected_columns]\n",
    "\n",
    "def upsample_to_common_timedelta(datasets, dfs, common_td):\n",
    "    dfs_resampled = []\n",
    "\n",
    "    for name, df in zip(datasets.keys(), dfs):\n",
    "        orig_td = pd.to_timedelta(datasets[name][\"timedelta\"], unit=\"m\")\n",
    "        if orig_td == common_td:\n",
    "            dfs_resampled.append(df)\n",
    "        else:\n",
    "            df = df.set_index(\"valid_dttm\")\n",
    "            df = df.resample(common_td).interpolate(method=\"linear\")\n",
    "            df = df.reset_index()\n",
    "            dfs_resampled.append(df)\n",
    "\n",
    "    return dfs_resampled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c68d5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Main loop over datasets, abort if no data in range\n",
    "dfs = []\n",
    "timedeltas = []\n",
    "\n",
    "for ds_name, ds_info in datasets.items():\n",
    "    print(f\"Processing {ds_name} from DOI: {ds_info['doi']}\")\n",
    "    doi = ds_info[\"doi\"]\n",
    "    timedelta_minutes = ds_info[\"timedelta\"]\n",
    "    timedeltas.append(timedelta_minutes)\n",
    "\n",
    "    variable_map = {k: v for k, v in ds_info[\"variables\"].items() if v is not None}\n",
    "\n",
    "    # Fetch and process\n",
    "    df_raw = fetch_flux_data(doi)\n",
    "    df_processed = process_data(df_raw, variable_map, station_info, start_date, end_date)\n",
    "\n",
    "    # Abort if no data in the specified time window\n",
    "    if df_processed.empty:\n",
    "        raise RuntimeError(\n",
    "            f\"❌ No data found in the time window ({start_date} to {end_date}) \"\n",
    "            f\"for dataset {ds_name} (DOI: {doi}). Aborting.\"\n",
    "        )\n",
    "\n",
    "    dfs.append(df_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4fa958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Harmonize resolution\n",
    "common_td = pd.to_timedelta(min(timedeltas), unit=\"m\")  # Choose finest resolution\n",
    "dfs_resampled = upsample_to_common_timedelta(datasets, dfs, common_td)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2351934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Merge all datasets\n",
    "from functools import reduce\n",
    "df_merged = reduce(lambda left, right: pd.merge(left, right, on=['valid_dttm', 'SID', 'lat', 'lon', 'elev'], how='outer'), dfs_resampled)\n",
    "df_merged = df_merged.sort_values(\"valid_dttm\").reset_index(drop=True)\n",
    "# Convert to Unix timestamp in seconds\n",
    "df_merged[\"valid_dttm\"] = pd.to_datetime(df_merged[\"valid_dttm\"], utc=True)\n",
    "df_merged[\"valid_dttm\"] = df_merged[\"valid_dttm\"].apply(lambda x: int(x.timestamp()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fddfa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Save to SQLite\n",
    "year = pd.to_datetime(start_date).year\n",
    "output_dir = f\"sqlites/data/observations/{station_info['Station_name']}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, f\"OBSTABLE_{year}.sqlite\")\n",
    "\n",
    "with sqlite3.connect(output_file) as conn:\n",
    "    df_merged.to_sql(\"SYNOP\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "    conn.execute(\"DROP TABLE IF EXISTS tmp\")\n",
    "    columns = ',\\n'.join([f\"{col} REAL\" for col in df_merged.columns])\n",
    "    conn.execute(f\"CREATE TABLE tmp ({columns})\")\n",
    "    conn.execute(\"INSERT INTO tmp SELECT * FROM SYNOP\")\n",
    "    conn.execute(\"DROP TABLE SYNOP\")\n",
    "    conn.execute(\"ALTER TABLE tmp RENAME TO SYNOP\")\n",
    "\n",
    "print(f\"✅ Data written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5472fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
