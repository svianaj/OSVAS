{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b3153d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import time\n",
    "import datetime\n",
    "from icoscp.cpb.dobj import Dobj\n",
    "import os\n",
    "from icoscp_core.icos import bootstrap\n",
    "from icoscp import cpauth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264b05de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Input parameters (edit these)\n",
    "OSVAS='/perm/sp3c/OSVAS/'\n",
    "station_name = \"Majadas_south\"\n",
    "start_date = \"2016-06-01\"\n",
    "end_date = \"2016-07-31\"\n",
    "station_list_path = os.path.join(OSVAS,\"./sqlites/station_list_SURFEX.csv\")\n",
    "dataset_doi = \"https://meta.icos-cp.eu/objects/dDlpnhS3XKyZjB22MUzP_nAm\"\n",
    "\n",
    "#In the following dictionary, select the variable names from dataset_doi \n",
    "#and how they will be renamed in the output sqlite\n",
    "\n",
    "variables = {'H_F_MDS': 'H', 'LE_F_MDS': 'LE'}\n",
    "\n",
    "#Authenticate into ICOS:\n",
    "cookie_file_path = os.path.join(OSVAS,\"./sqlites/icos_cookie.txt\")\n",
    "cookie_token=open(cookie_file_path,'r').readline().rstrip()\n",
    "meta, data = bootstrap.fromCookieToken(cookie_token)\n",
    "cpauth.init_by(data.auth)\n",
    "\n",
    "#Test: If the authentication went well, these lines of code will not fail:\n",
    "import icoscp\n",
    "from icoscp.dobj import Dobj\n",
    "obj_flux='https://meta.icos-cp.eu/objects/dDlpnhS3XKyZjB22MUzP_nAm'\n",
    "dobj_flux=Dobj(obj_flux).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de0029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define helper functions\n",
    "def load_station_metadata(path):\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def get_station_info(name, metadata_df):\n",
    "    row = metadata_df[metadata_df['name'] == name]\n",
    "    if row.empty:\n",
    "        raise ValueError(f\"Station '{name}' not found in the metadata file.\")\n",
    "    return row.iloc[0]\n",
    "\n",
    "def fetch_flux_data(doi, cookie_token):\n",
    "    # Bootstrap session using your token\n",
    "    meta, data = bootstrap.fromCookieToken(cookie_token)\n",
    "    cpauth.init_by(data.auth)\n",
    "\n",
    "    # Fetch the data object\n",
    "    dobj = Dobj(doi)\n",
    "    df = dobj.data\n",
    "    return df\n",
    "\n",
    "\n",
    "def fetch_flux_data(doi):\n",
    "    dobj = Dobj(doi)\n",
    "    df = dobj.data\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_data(df, station_info, start, end):\n",
    "    df['valid_dttm'] = pd.to_datetime(df['TIMESTAMP'], utc=True)\n",
    "    df = df[(df['valid_dttm'] >= start) & (df['valid_dttm'] <= end)]\n",
    "    df['valid_dttm'] = df['TIMESTAMP'].view('int64')\n",
    "    df = df.dropna(subset=['H_F_MDS', 'LE_F_MDS'])\n",
    "    df['SID'] = station_info['SID']\n",
    "    df['lat'] = station_info['lat']\n",
    "    df['lon'] = station_info['lon']\n",
    "    df['elev'] = station_info['elev']\n",
    "    return df[['valid_dttm', 'SID', 'lat', 'lon', 'elev', 'H_F_MDS', 'LE_F_MDS']]\n",
    "\n",
    "def process_data(df, station_info, start, end, variable_names):\n",
    "    \"\"\"\n",
    "    Processes a DataFrame, selecting data within a time range, adding station information,\n",
    "    and renaming specified columns based on a dictionary.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame with a 'TIMESTAMP' column and the variables to process.\n",
    "        station_info (dict): A dictionary containing station information with keys 'SID', 'lat', 'lon', and 'elev'.\n",
    "        start (str or datetime): The start timestamp for filtering.\n",
    "        end (str or datetime): The end timestamp for filtering.\n",
    "        variable_names (dict): A dictionary where keys are the original column names in df\n",
    "                               and values are the desired new column names.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the processed data with renamed columns.\n",
    "    \"\"\"\n",
    "    df['valid_dttm'] = pd.to_datetime(df['TIMESTAMP'], utc=True)\n",
    "\n",
    "    # Filter to desired datetime range\n",
    "    df = df[(df['valid_dttm'] >= start) & (df['valid_dttm'] <= end)].copy()\n",
    "\n",
    "    # Convert to Unix time in seconds\n",
    "    df['valid_dttm'] = df['valid_dttm'].astype('int64')/1000\n",
    "\n",
    "\n",
    "    # Select and drop rows with NaN values for the specified variables\n",
    "    variables_to_process = list(variable_names.keys())\n",
    "    df = df.dropna(subset=variables_to_process).copy()\n",
    "\n",
    "    df['SID'] = station_info['SID']\n",
    "    df['lat'] = station_info['lat']\n",
    "    df['lon'] = station_info['lon']\n",
    "    df['elev'] = station_info['elev']\n",
    "\n",
    "    # Select the desired columns and rename them\n",
    "    columns_to_select = ['valid_dttm', 'SID', 'lat', 'lon', 'elev'] + variables_to_process\n",
    "    df_processed = df[columns_to_select].rename(columns=variable_names)\n",
    "\n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46edda42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load metadata and fetch data\n",
    "station_metadata = load_station_metadata(station_list_path)\n",
    "station_info = get_station_info(station_name, station_metadata)\n",
    "print(f\"Loaded metadata for station: {station_name}\")\n",
    "print(station_info)\n",
    "df_raw = fetch_flux_data(dataset_doi)\n",
    "#df_raw = fetch_flux_data(dataset_doi, cookie_file_path)\n",
    "print(\"Fetched data from ICOS.\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e588a36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Process and preview final data\n",
    "df_processed = process_data(df_raw, station_info, start_date, end_date, variables)\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fede7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Write to SQLite\n",
    "year = pd.to_datetime(start_date).year\n",
    "output_dir = os.path.join(OSVAS,\"sqlites/data/observations\",station_name)\n",
    "os.makedirs(output_dir, exist_ok=True) # Create directory if it doesn't exist\n",
    "output_file = os.path.join(output_dir, f\"OBSTABLE_{year}.sqlite\")\n",
    "\n",
    "\n",
    "with sqlite3.connect(output_file) as conn:\n",
    "    df_processed.to_sql(\"SYNOP\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "    conn.execute(\"DROP TABLE IF EXISTS tmp\")\n",
    "    conn.execute(\"\"\"\n",
    "        CREATE TABLE tmp (\n",
    "            valid_dttm REAL,\n",
    "            SID REAL,\n",
    "            lat REAL,\n",
    "            lon REAL,\n",
    "            elev REAL,\n",
    "            H REAL,\n",
    "            LE REAL\n",
    "        )\n",
    "    \"\"\")\n",
    "    conn.execute(\"INSERT INTO tmp SELECT * FROM SYNOP\")\n",
    "    conn.execute(\"DROP TABLE SYNOP\")\n",
    "    conn.execute(\"ALTER TABLE tmp RENAME TO SYNOP\")\n",
    "\n",
    "print(f\"Data written to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.10-01",
   "language": "python",
   "name": "python-3.11.10-01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
