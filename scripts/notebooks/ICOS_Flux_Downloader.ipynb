{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f731e79-7fb0-4f11-990b-111c8259a42b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T15:38:24.969835Z",
     "iopub.status.busy": "2025-11-14T15:38:24.969694Z",
     "iopub.status.idle": "2025-11-14T15:38:24.972746Z",
     "shell.execute_reply": "2025-11-14T15:38:24.972402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Validation files for: Majadas_del_tietar with OSVAS installation in /home/pn56/OSVASgh/\n"
     ]
    }
   ],
   "source": [
    "###### OSVAS ########################################################\n",
    "###### ( OFFLINE SURFEX VALIDATION SYSTEM)###########################\n",
    "###### STEP 2: Downloading validation data from ICOS #############\n",
    "#### STEP 2.0: DEFINING STATION and OSVAS PATH ########################\n",
    "import os\n",
    "# Default values defined in the notebook for Station and OSVAS install:\n",
    "OSVAS='/home/pn56/OSVASgh/'  # Main OSVAS path\n",
    "Station_name='Loobos'\n",
    "\n",
    "# If an environment variable STATION or OSVAS exists, override the default\n",
    "Station_name = os.getenv(\"STATION_NAME\", Station_name)\n",
    "OSVAS = os.getenv(\"OSVAS\", OSVAS)\n",
    "\n",
    "print(f\"Creating Validation files for: {Station_name} with OSVAS installation in {OSVAS}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b60736d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T15:38:24.974394Z",
     "iopub.status.busy": "2025-11-14T15:38:24.974281Z",
     "iopub.status.idle": "2025-11-14T15:38:25.237022Z",
     "shell.execute_reply": "2025-11-14T15:38:25.236517Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_156756/3899529518.py:9: UserWarning: \n",
      "It is highly recommended to replace the following import:\n",
      "\"from icoscp.cpb.dobj import Dobj\"\n",
      "  with\n",
      "\"from icoscp.dobj import Dobj\"\n",
      "Find out more here: https://icos-carbon-portal.github.io/pylib/icoscp/install/#upgrade-guide\n",
      "  from icoscp.cpb.dobj import Dobj\n"
     ]
    }
   ],
   "source": [
    "###### OSVAS ##################################################################\n",
    "###### ( OFFLINE SURFEX VALIDATION SYSTEM)#####################################\n",
    "#### STEP 2.1: IMPORTING NEEDED PACKAGES AND DEFINING FUNCTIONS ###############\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import os\n",
    "import yaml\n",
    "from icoscp.cpb.dobj import Dobj\n",
    "from icoscp_core.icos import bootstrap\n",
    "from icoscp import cpauth\n",
    "\n",
    "def fetch_flux_data(doi):\n",
    "    dobj = Dobj(doi)\n",
    "    df = dobj.data\n",
    "    return df\n",
    "\n",
    "def process_data(df, variable_map, station_info, start, end):\n",
    "    df['valid_dttm'] = pd.to_datetime(df['TIMESTAMP'], utc=True)\n",
    "    df = df[(df['valid_dttm'] >= start) & (df['valid_dttm'] <= end)].copy()\n",
    "\n",
    "    # Drop rows with missing required vars\n",
    "    source_vars = list(variable_map.values())\n",
    "    df = df.dropna(subset=source_vars)\n",
    "\n",
    "    # Add station metadata\n",
    "    df[\"SID\"] = int(station_info[\"SID\"])\n",
    "    df[\"SID\"] = df[\"SID\"].astype(\"Int64\")  # optional if you want pandas nullable integer type\n",
    "    df['lat'] = station_info['lat']\n",
    "    df['lon'] = station_info['lon']\n",
    "    df['elev'] = station_info['elev']\n",
    "\n",
    "    # Rename variables\n",
    "    df = df.rename(columns={v: k for k, v in variable_map.items()})\n",
    "    selected_columns = ['valid_dttm', 'SID', 'lat', 'lon', 'elev'] + list(variable_map.keys())\n",
    "\n",
    "    return df[selected_columns]\n",
    "\n",
    "def upsample_to_common_timedelta(datasets, dfs, common_td):\n",
    "    dfs_resampled = []\n",
    "\n",
    "    for name, df in zip(datasets.keys(), dfs):\n",
    "        orig_td = pd.to_timedelta(datasets[name][\"timedelta\"], unit=\"m\")\n",
    "        if orig_td == common_td:\n",
    "            dfs_resampled.append(df)\n",
    "        else:\n",
    "            df = df.set_index(\"valid_dttm\")\n",
    "            df = df.resample(common_td).interpolate(method=\"linear\")\n",
    "            df = df.reset_index()\n",
    "            dfs_resampled.append(df)\n",
    "\n",
    "    return dfs_resampled\n",
    "\n",
    "def _to_datetime_series(s):\n",
    "    \"\"\"Return datetime64[ns,UTC] series for index or column 'valid_dttm'.\"\"\"\n",
    "    # If already datetime\n",
    "    if pd.api.types.is_datetime64_any_dtype(s):\n",
    "        return pd.to_datetime(s).dt.tz_convert('UTC') if s.dt.tz is not None else pd.to_datetime(s).dt.tz_localize('UTC')\n",
    "    # If numeric -> assume epoch seconds\n",
    "    if pd.api.types.is_numeric_dtype(s):\n",
    "        return pd.to_datetime(s, unit='s', utc=True)\n",
    "    # Else try parse strings\n",
    "    return pd.to_datetime(s, utc=True)\n",
    "\n",
    "def enforce_seb_closure(df,\n",
    "                        closure=1,\n",
    "                        timestamp_col='valid_dttm'):\n",
    "    \"\"\"\n",
    "    Enforce surface energy balance closure on df, returning a copy with H_cor and LE_cor.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Must contain columns: H, LE, SW_IN, SW_OUT, LW_IN, LW_OUT\n",
    "        Optionally: G, S_can, S_T, S_veg, S_q\n",
    "    closure : int (1,2,3,4)\n",
    "        1: timestep BR (old-style)\n",
    "        2: daily BR (use daily sums H and LE to compute BR, then apply per-timestep)\n",
    "        3: timestep BR, include S_can in SEB (H+LE = Rn - G - S_can)\n",
    "        4: timestep BR with S_can in SEB and BR uses canopy components:\n",
    "           BR = (H + S_T + S_veg) / (LE + S_q)\n",
    "    timestamp_col : str\n",
    "        Column name holding timestamps (datetime or epoch seconds). Used only for closure==2.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    df_out : pd.DataFrame (copy of input with added columns)\n",
    "        Columns added: AE, BR_used, H_cor, LE_cor\n",
    "    \"\"\"\n",
    "    df_out = df.copy()\n",
    "    \n",
    "    # Ensure timestamps\n",
    "    if timestamp_col in df_out.columns:\n",
    "        tseries = _to_datetime_series(df_out[timestamp_col])\n",
    "        df_out['_dt_index_for_daily'] = tseries.dt.floor('D')  # used for closure==2\n",
    "    else:\n",
    "        # create a dummy index if no timestamp column\n",
    "        df_out['_dt_index_for_daily'] = pd.NaT\n",
    "\n",
    "    # Required radiation columns\n",
    "    for col in ['SW_IN', 'SW_OUT', 'LW_IN', 'LW_OUT', 'H', 'LE']:\n",
    "        if col not in df_out.columns:\n",
    "            raise KeyError(f\"Required column '{col}' missing from dataframe\")\n",
    "\n",
    "    # compute net radiation\n",
    "    df_out['Rn'] = df_out['SW_IN'] - df_out['SW_OUT'] + df_out['LW_IN'] - df_out['LW_OUT']\n",
    "\n",
    "    # ground heat flux G (if not present assume 0)\n",
    "    df_out['G'] = df_out['G'] if 'G' in df_out.columns else 0.0\n",
    "\n",
    "    # canopy storage S_can: prefer S_can column; else sum components if present; else 0.\n",
    "    if 'S_can' in df_out.columns:\n",
    "        df_out['S_can'] = df_out['S_can'].fillna(0.0)\n",
    "    else:\n",
    "        # component names may or may not exist\n",
    "        sT = df_out['S_T'] if 'S_T' in df_out.columns else 0.0\n",
    "        sveg = df_out['S_veg'] if 'S_veg' in df_out.columns else 0.0\n",
    "        sq = df_out['S_q'] if 'S_q' in df_out.columns else 0.0\n",
    "        # If any of these exist as Series, ensure fillna(0)\n",
    "        def _maybe_fill(x):\n",
    "            return x.fillna(0.0) if isinstance(x, pd.Series) else x\n",
    "        df_out['S_can'] = _maybe_fill(sT) + _maybe_fill(sveg) + _maybe_fill(sq)\n",
    "\n",
    "    # Available energy AE depending on closure mode:\n",
    "    # closure 1 & 2: AE = Rn - G\n",
    "    # closure 3 & 4: AE = Rn - G - S_can\n",
    "    if closure in (1, 2):\n",
    "        df_out['AE'] = df_out['Rn'] - df_out['G']\n",
    "    elif closure in (3, 4):\n",
    "        df_out['AE'] = df_out['Rn'] - df_out['G'] - df_out['S_can']\n",
    "    else:\n",
    "        raise ValueError(\"closure must be 1,2,3 or 4\")\n",
    "\n",
    "    # safe helper for division with fallback\n",
    "    def compute_BR_timestep(Hs, LEs):\n",
    "        \"\"\"Return BR series H/LE with safe handling\"\"\"\n",
    "        Hs = pd.Series(Hs).astype(float)\n",
    "        LEs = pd.Series(LEs).astype(float)\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            br = Hs / LEs\n",
    "        # When both zero -> set BR to 1 (split equally). When denominator zero but numerator nonzero -> large value\n",
    "        mask_both_zero = (Hs == 0) & (LEs == 0)\n",
    "        br.loc[mask_both_zero] = 1.0\n",
    "        # When LE == 0 and H != 0, we keep br as is (inf) — will be handled later numerically\n",
    "        return br\n",
    "\n",
    "    # Determine BR_used per closure option\n",
    "    if closure == 1:\n",
    "        BR_used = compute_BR_timestep(df_out['H'], df_out['LE'])\n",
    "\n",
    "    elif closure == 2:\n",
    "        # daily BR computed from daily sums\n",
    "        if '_dt_index_for_daily' not in df_out.columns:\n",
    "            raise KeyError(\"Timestamp column required for closure==2\")\n",
    "        grouped = df_out.groupby('_dt_index_for_daily')[['H', 'LE']].sum(min_count=1)\n",
    "        # compute daily BR safely\n",
    "        BR_daily = compute_BR_timestep(grouped['H'], grouped['LE'])\n",
    "        # map daily BR back to rows\n",
    "        BR_used = df_out['_dt_index_for_daily'].map(BR_daily)\n",
    "        # if some days missing (NaN) fallback to timestep BR for those rows\n",
    "        br_tstep = compute_BR_timestep(df_out['H'], df_out['LE'])\n",
    "        BR_used = BR_used.fillna(br_tstep)\n",
    "\n",
    "    elif closure == 3:\n",
    "        # same as 1 but AE already included S_can\n",
    "        BR_used = compute_BR_timestep(df_out['H'], df_out['LE'])\n",
    "\n",
    "    elif closure == 4:\n",
    "        # BR = (H + S_T + S_veg) / (LE + S_q)\n",
    "        # collect components, default 0 if missing\n",
    "        S_T = df_out['S_T'] if 'S_T' in df_out.columns else 0.0\n",
    "        S_veg = df_out['S_veg'] if 'S_veg' in df_out.columns else 0.0\n",
    "        S_q = df_out['S_q'] if 'S_q' in df_out.columns else 0.0\n",
    "        # ensure Series with fillna\n",
    "        def _to_series_or_const(x):\n",
    "            return x.fillna(0.0) if isinstance(x, pd.Series) else x\n",
    "        a = _to_series_or_const(S_T) + _to_series_or_const(S_veg)   # added to H numerator\n",
    "        b = _to_series_or_const(S_q)                                # added to LE denominator\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            BR_used = (df_out['H'] + a) / (df_out['LE'] + b)\n",
    "        # if both numerator and denominator zero, fallback to 1\n",
    "        mask_both_zero = ((df_out['H'] + a) == 0) & ((df_out['LE'] + b) == 0)\n",
    "        BR_used.loc[mask_both_zero] = 1.0\n",
    "    else:\n",
    "        raise ValueError(\"closure must be 1..4\")\n",
    "\n",
    "    # store BR_used\n",
    "    df_out['BR_used'] = BR_used.astype(float)\n",
    "\n",
    "    # Solve for H_cor and LE_cor\n",
    "    # general solution for closure types 1-3:\n",
    "    #  Hc + Lec = AE\n",
    "    #  Hc / Lec = BR  =>  Hc = AE * BR / (1 + BR) ; Lec = AE / (1 + BR)\n",
    "    # For numerical stability, transform when BR is inf or very large or negative\n",
    "    Hc = pd.Series(index=df_out.index, dtype=float)\n",
    "    Lec = pd.Series(index=df_out.index, dtype=float)\n",
    "    AE = df_out['AE'].astype(float)\n",
    "    BR = df_out['BR_used'].astype(float)\n",
    "\n",
    "    # handle closure 4 separately because BR definition includes storage terms and solution differs:\n",
    "    if closure != 4:\n",
    "        # Avoid dividing by (1+BR) issues. Use safe formula:\n",
    "        denom = 1.0 + BR\n",
    "        # If denom is zero (BR == -1) -> can't use ratio formula; fallback to distribute AE proportionally to absolute magnitudes\n",
    "        mask_bad_denom = np.isclose(denom, 0.0) | (~np.isfinite(denom))\n",
    "        # regular cases\n",
    "        mask_regular = ~mask_bad_denom\n",
    "        Hc.loc[mask_regular] = AE[mask_regular] * BR[mask_regular] / denom[mask_regular]\n",
    "        Lec.loc[mask_regular] = AE[mask_regular] / denom[mask_regular]\n",
    "\n",
    "        # fallback for bad denom or NaN BR: distribute AE using observed H and LE proportions\n",
    "        mask_fallback = mask_bad_denom | (~np.isfinite(BR))\n",
    "        if mask_fallback.any():\n",
    "            # proportions from observed magnitudes; use absolute because signs can cancel\n",
    "            sum_obs = (np.abs(df_out.loc[mask_fallback, 'H']) + np.abs(df_out.loc[mask_fallback, 'LE']))\n",
    "            # if sum_obs == 0 -> split equally\n",
    "            eq_mask = sum_obs == 0\n",
    "            if eq_mask.any():\n",
    "                Hc.loc[mask_fallback][eq_mask] = 0.5 * AE.loc[mask_fallback][eq_mask]\n",
    "                Lec.loc[mask_fallback][eq_mask] = 0.5 * AE.loc[mask_fallback][eq_mask]\n",
    "            # else distribute proportionally\n",
    "            non_eq_mask = ~eq_mask\n",
    "            if non_eq_mask.any():\n",
    "                p = np.abs(df_out.loc[mask_fallback, 'H']) / sum_obs\n",
    "                Hc.loc[mask_fallback][non_eq_mask] = AE.loc[mask_fallback][non_eq_mask] * p[non_eq_mask]\n",
    "                Lec.loc[mask_fallback][non_eq_mask] = AE.loc[mask_fallback][non_eq_mask] * (1.0 - p[non_eq_mask])\n",
    "\n",
    "    else:\n",
    "        # closure == 4: solve linear system with canopy terms\n",
    "        # System:\n",
    "        #   Hc + Lec = AE\n",
    "        #   (Hc + a) = BR*(Lec + b)  => Hc - BR*Lec = BR*b - a\n",
    "        # Solve:\n",
    "        # From Hc = AE - Lec -> AE - Lec - BR*Lec = BR*b - a\n",
    "        # => Lec * (1 + BR) = AE - (BR*b - a)\n",
    "        # => Lec = (AE - (BR*b - a)) / (1 + BR)\n",
    "        S_T = df_out['S_T'] if 'S_T' in df_out.columns else 0.0\n",
    "        S_veg = df_out['S_veg'] if 'S_veg' in df_out.columns else 0.0\n",
    "        S_q = df_out['S_q'] if 'S_q' in df_out.columns else 0.0\n",
    "        a = (S_T if isinstance(S_T, (int,float)) else S_T.fillna(0.0)) + (S_veg if isinstance(S_veg, (int,float)) else S_veg.fillna(0.0))\n",
    "        b = S_q if isinstance(S_q, (int,float)) else S_q.fillna(0.0)\n",
    "\n",
    "        denom = 1.0 + BR\n",
    "        # regular cases\n",
    "        mask_regular = ~np.isclose(denom, 0.0) & np.isfinite(denom)\n",
    "        if mask_regular.any():\n",
    "            Lec.loc[mask_regular] = (AE.loc[mask_regular] - (BR.loc[mask_regular] * b.loc[mask_regular] - a.loc[mask_regular])) / denom.loc[mask_regular]\n",
    "            Hc.loc[mask_regular] = AE.loc[mask_regular] - Lec.loc[mask_regular]\n",
    "\n",
    "        # fallback if denom zero or invalid: distribute AE by observed proportions as before\n",
    "        mask_fallback = ~mask_regular\n",
    "        if mask_fallback.any():\n",
    "            sum_obs = (np.abs(df_out.loc[mask_fallback, 'H']) + np.abs(df_out.loc[mask_fallback, 'LE']))\n",
    "            eq_mask = sum_obs == 0\n",
    "            if eq_mask.any():\n",
    "                Hc.loc[mask_fallback][eq_mask] = 0.5 * AE.loc[mask_fallback][eq_mask]\n",
    "                Lec.loc[mask_fallback][eq_mask] = 0.5 * AE.loc[mask_fallback][eq_mask]\n",
    "            non_eq_mask = ~eq_mask\n",
    "            if non_eq_mask.any():\n",
    "                p = np.abs(df_out.loc[mask_fallback, 'H']) / sum_obs\n",
    "                Hc.loc[mask_fallback][non_eq_mask] = AE.loc[mask_fallback][non_eq_mask] * p[non_eq_mask]\n",
    "                Lec.loc[mask_fallback][non_eq_mask] = AE.loc[mask_fallback][non_eq_mask] * (1.0 - p[non_eq_mask])\n",
    "\n",
    "    # attach to df_out\n",
    "    df_out['H_cor'] = Hc\n",
    "    df_out['LE_cor'] = Lec\n",
    "\n",
    "    # cleanup helper column if created\n",
    "    if '_dt_index_for_daily' in df_out.columns:\n",
    "        df_out.drop(columns=['_dt_index_for_daily'], inplace=True)\n",
    "\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af27ef1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T15:38:25.238388Z",
     "iopub.status.busy": "2025-11-14T15:38:25.238242Z",
     "iopub.status.idle": "2025-11-14T15:38:26.776355Z",
     "shell.execute_reply": "2025-11-14T15:38:26.775718Z"
    }
   },
   "outputs": [],
   "source": [
    "###### OSVAS ###################################################################\n",
    "###### ( OFFLINE SURFEX VALIDATION SYSTEM)######################################\n",
    "#### STEP 2.2: LOAD STATION METADATA AND CONFIGURATION OF  #####################\n",
    "#### THE GENERATION OF VALIDATION SQLITES FROM THE STATION'S YAML FILE #########\n",
    "\n",
    "os.chdir(OSVAS)\n",
    "CONFIG_PATH = f\"config_files/Stations/{Station_name}.yml\"\n",
    "\n",
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "station_info = config[\"Station_metadata\"]\n",
    "validation_data = config[\"Validation_data\"]\n",
    "common_obstable = validation_data.get(\"common_obstable\", False)\n",
    "\n",
    "start_date = pd.to_datetime(validation_data[\"validation_start\"], utc=True)\n",
    "end_date = pd.to_datetime(validation_data[\"validation_end\"], utc=True)\n",
    "\n",
    "closure_type = config.get(\"Station_metadata\", {}).get(\"closure_type\", 1) #Default value is 1.\n",
    "\n",
    "datasets = {k: v for k, v in validation_data.items() if k.startswith(\"dataset\") or k.startswith(\"dataset_\")}\n",
    "\n",
    "# Get access to ICOS\n",
    "# Authenticate using cookie (adjust if needed)\n",
    "cookie_path = \"icos_cookie.txt\"  # Or point to config\n",
    "cookie_token = open(cookie_path, \"r\").readline().strip()\n",
    "meta, data = bootstrap.fromCookieToken(cookie_token)\n",
    "cpauth.init_by(data.auth)\n",
    "\n",
    "#Test: If the authentication went well, these lines of code will not fail:\n",
    "import icoscp\n",
    "from icoscp.dobj import Dobj\n",
    "obj_flux='https://meta.icos-cp.eu/objects/dDlpnhS3XKyZjB22MUzP_nAm'\n",
    "dobj_flux=Dobj(obj_flux).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c68d5fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T15:38:26.777751Z",
     "iopub.status.busy": "2025-11-14T15:38:26.777619Z",
     "iopub.status.idle": "2025-11-14T15:38:29.052306Z",
     "shell.execute_reply": "2025-11-14T15:38:29.051934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset1 from DOI: https://meta.icos-cp.eu/objects/fPAqntOb1uiTQ2KI1NS1CHlB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset2 from DOI: https://meta.icos-cp.eu/objects/tONKGY9pOYqVInayCYac-4LI\n"
     ]
    }
   ],
   "source": [
    "###### OSVAS ###################################################################\n",
    "###### ( OFFLINE SURFEX VALIDATION SYSTEM)######################################\n",
    "#### STEP 2.3: Main loop over datasets, abort if no data in range ##############\n",
    "\n",
    "dfs = []\n",
    "timedeltas = []\n",
    "units_map = {}  # <-- Dict that will be filled with var/units pairs from all datasets\n",
    "for ds_name, ds_info in datasets.items():\n",
    "    print(f\"Processing {ds_name} from DOI: {ds_info['doi']}\")\n",
    "    doi = ds_info[\"doi\"]\n",
    "    timedelta_minutes = ds_info[\"timedelta\"]\n",
    "    timedeltas.append(timedelta_minutes)\n",
    "\n",
    "    variable_map = {k: v for k, v in ds_info[\"variables\"].items() if v is not None}\n",
    "    units_map.update({k: v for k, v in ds_info[\"units\"].items() if v is not None})\n",
    "    # Fetch and process\n",
    "    df_raw = fetch_flux_data(doi)\n",
    "    df_processed = process_data(df_raw, variable_map, station_info, start_date, end_date)\n",
    "\n",
    "    # Abort if no data in the specified time window\n",
    "    if df_processed.empty:\n",
    "        raise RuntimeError(\n",
    "            f\"❌ No data found in the time window ({start_date} to {end_date}) \"\n",
    "            f\"for dataset {ds_name} (DOI: {doi}). Aborting.\"\n",
    "        )\n",
    "\n",
    "    dfs.append(df_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e4fa958",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T15:38:29.054161Z",
     "iopub.status.busy": "2025-11-14T15:38:29.053952Z",
     "iopub.status.idle": "2025-11-14T15:38:29.061251Z",
     "shell.execute_reply": "2025-11-14T15:38:29.060729Z"
    }
   },
   "outputs": [],
   "source": [
    "###### OSVAS ############################################################################\n",
    "###### ( OFFLINE SURFEX VALIDATION SYSTEM)###############################################\n",
    "#### STEP 2.4: Harmonize resolution, merge datasets, apply SEB closure if needed ########\n",
    "common_td = pd.to_timedelta(min(timedeltas), unit=\"m\")  # Choose finest resolution\n",
    "dfs_resampled = upsample_to_common_timedelta(datasets, dfs, common_td)\n",
    "# Cell 6: Merge all datasets, produce Hcor and LEcor based in a closure method if necessary and all SEB components available.\n",
    "from functools import reduce\n",
    "df_merged = reduce(lambda left, right: pd.merge(left, right, on=['valid_dttm', 'SID', 'lat', 'lon', 'elev'], how='outer'), dfs_resampled)\n",
    "df_merged = df_merged.sort_values(\"valid_dttm\").reset_index(drop=True)\n",
    "\n",
    "#df_merged=enforce_seb_closure(df_merged,closure_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f52233a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T15:38:29.063063Z",
     "iopub.status.busy": "2025-11-14T15:38:29.062909Z",
     "iopub.status.idle": "2025-11-14T15:38:29.110544Z",
     "shell.execute_reply": "2025-11-14T15:38:29.110109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Year 2021 data merged into sqlites/validation_data/Majadas_del_tietar/OBSTABLE_2021.sqlite\n"
     ]
    }
   ],
   "source": [
    "###### OSVAS ############################################################################\n",
    "###### ( OFFLINE SURFEX VALIDATION SYSTEM)###############################################\n",
    "#### STEP 2.5: Convert to Unix timestamp in seconds and save dataframe to SQLite#########\n",
    "\n",
    "import sqlite3\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1️⃣ Preprocess dataframe ---\n",
    "df_merged[\"valid_dttm\"] = pd.to_datetime(df_merged[\"valid_dttm\"], utc=True)\n",
    "df_merged[\"year_obs\"] =df_merged[\"valid_dttm\"].dt.year  # extract year for splitting\n",
    "df_merged[\"valid_dttm\"] = df_merged[\"valid_dttm\"].apply(lambda x: int(x.timestamp()))\n",
    "#df_merged[\"valid_dttm\"] =_to_datetime_series(df_merged[\"valid_dttm\"])\n",
    "\n",
    "output_dir = (\n",
    "    \"sqlites/validation_data/common_obstables\"\n",
    "    if common_obstable\n",
    "    else f\"sqlites/validation_data/{station_info['Station_name']}\"\n",
    ")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# --- 2️⃣ Loop over years ---\n",
    "for year, df_year in df_merged.groupby(\"year_obs\"):\n",
    "    output_file = os.path.join(output_dir, f\"OBSTABLE_{year}.sqlite\")\n",
    "\n",
    "    with sqlite3.connect(output_file) as conn:\n",
    "        incoming_cols = list(df_year.columns)\n",
    "\n",
    "        # --- 1️⃣ Build CREATE TABLE statement with correct types ---\n",
    "        col_defs = []\n",
    "        for c in incoming_cols:\n",
    "            if c in (\"SID\",\"valid_dttm\"):\n",
    "                col_defs.append(f'\"{c}\" INTEGER')\n",
    "            else:\n",
    "                col_defs.append(f'\"{c}\" REAL')\n",
    "\n",
    "        conn.execute(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS SYNOP (\n",
    "                {\", \".join(col_defs)},\n",
    "                UNIQUE(\"valid_dttm\",\"SID\")\n",
    "            );\n",
    "        \"\"\")\n",
    "\n",
    "        # --- 2️⃣ Detect existing columns ---\n",
    "        existing_cols = [row[1] for row in conn.execute(\"PRAGMA table_info(SYNOP);\")]\n",
    "\n",
    "        # --- 3️⃣ Add new columns as REAL (except SID, which should already exist) ---\n",
    "        for col in incoming_cols:\n",
    "            if col not in existing_cols:\n",
    "                if col in (\"SID\",\"valid_dttm\"):\n",
    "                    conn.execute(f'ALTER TABLE SYNOP ADD COLUMN \"{col}\" INTEGER;')\n",
    "                else:\n",
    "                    conn.execute(f'ALTER TABLE SYNOP ADD COLUMN \"{col}\" REAL;')\n",
    "\n",
    "        # --- 4️⃣ Refresh column list ---\n",
    "        existing_cols = [row[1] for row in conn.execute(\"PRAGMA table_info(SYNOP);\")]\n",
    "\n",
    "        # --- 5️⃣ Fill any missing columns in df ---\n",
    "        for col in existing_cols:\n",
    "            if col not in df_year.columns:\n",
    "                df_year[col] = None\n",
    "\n",
    "        # --- 6️⃣ Enforce integer type for SID before writing ---\n",
    "        if \"SID\" in df_year.columns:\n",
    "            df_year[\"SID\"] = pd.to_numeric(df_year[\"SID\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "        df_year = df_year[existing_cols]\n",
    "\n",
    "        # --- 7️⃣ Write to temporary table ---\n",
    "        df_year.to_sql(\"SYNOP_tmp\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "        # --- 8️⃣ Merge logic ---\n",
    "        conn.execute(\"\"\"\n",
    "            DELETE FROM SYNOP\n",
    "            WHERE (valid_dttm, SID) IN (\n",
    "                SELECT valid_dttm, SID FROM SYNOP_tmp\n",
    "            );\n",
    "        \"\"\")\n",
    "\n",
    "        col_names = \", \".join([f'\"{c}\"' for c in existing_cols])\n",
    "        conn.execute(f\"\"\"\n",
    "            INSERT INTO SYNOP ({col_names})\n",
    "            SELECT {col_names} FROM SYNOP_tmp;\n",
    "        \"\"\")\n",
    "\n",
    "        conn.execute(\"DROP TABLE SYNOP_tmp\")\n",
    "        # ---- create SYNOP_params if missing ----\n",
    "        conn.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS SYNOP_params (\n",
    "                parameter VARCHAR PRIMARY KEY,\n",
    "                accum_hours REAL,\n",
    "                units VARCHAR\n",
    "            );\n",
    "        \"\"\")\n",
    "\n",
    "        # ---- get SYNOP columns from the actual table schema ----\n",
    "        synop_cols = [row[1] for row in conn.execute(\"PRAGMA table_info(SYNOP);\")]\n",
    "\n",
    "        # ---- define which columns are metadata and should NOT be listed in SYNOP_params ----\n",
    "        skip_cols = {\"SID\", \"valid_dttm\", \"lat\", \"lon\", \"elev\", \"year_obs\"}\n",
    "\n",
    "        # ---- find which parameter names are already present to avoid duplicates ----\n",
    "        existing_params = {row[0] for row in conn.execute(\"SELECT parameter FROM SYNOP_params;\")}\n",
    "\n",
    "        # ---- prepare rows for insertion: only column names in SYNOP that are not metadata and not already present ----\n",
    "        rows_to_insert = []\n",
    "        for col in synop_cols:\n",
    "            if col in skip_cols:\n",
    "                continue\n",
    "            if col in existing_params:\n",
    "                continue\n",
    "            unit = units_map.get(col, \"\")   # default to empty string if not in units_map\n",
    "            rows_to_insert.append((col, 0.0, unit))\n",
    "\n",
    "        # ---- insert missing parameter rows ----\n",
    "        if rows_to_insert:\n",
    "            conn.executemany(\n",
    "                \"INSERT INTO SYNOP_params (parameter, accum_hours, units) VALUES (?, ?, ?);\",\n",
    "                rows_to_insert\n",
    "            )\n",
    "            conn.commit()        \n",
    "\n",
    "    print(f\"✅ Year {year} data merged into {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183c1ff9-eeb9-41ff-ad30-7427ea8d07ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
