{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3102d619-e946-4f4e-bc17-fa7208a6a0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### OSVAS ########################################################\n",
    "###### ( OFFLINE SURFEX VALIDATION SYSTEM)###########################\n",
    "#### STEP 1: IMPORTING NEEDED PACKAGES AND DEFINING FUNCTIONS ###############\n",
    "import icoscp\n",
    "from icoscp.dobj import Dobj\n",
    "import sys\n",
    "sys.path.append(\"~/.local/lib/python3.10/site-packages/\")\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "from datetime import date, datetime, timedelta\n",
    "import os\n",
    "import time\n",
    "import metpy\n",
    "#import meteo\n",
    "#import meteo.humidity\n",
    "#import meteo.air\n",
    "# Note: meteo package available here: https://github.com/hendrikwout/meteo\n",
    "from pathlib import Path\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "from netCDF4 import Dataset, date2num\n",
    "import cftime\n",
    "from metpy.calc import (\n",
    "    mixing_ratio_from_relative_humidity,\n",
    "    specific_humidity_from_mixing_ratio,\n",
    "    saturation_vapor_pressure,\n",
    ")\n",
    "from metpy.units import units\n",
    "import yaml\n",
    "from icoscp.cpb.dobj import Dobj\n",
    "from icoscp_core.icos import bootstrap\n",
    "from icoscp import cpauth\n",
    "import re\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 500\n",
    "##############################################################################\n",
    "##Here comes a series of functions for handling the forcing creation easily ##\n",
    "##############################################################################\n",
    "def datespan(startDate, endDate, delta=timedelta(days=1)):\n",
    "    currentDate = startDate\n",
    "    while currentDate < endDate:\n",
    "        yield currentDate\n",
    "        currentDate += delta\n",
    "\n",
    "def despike(pandas_column, sigma_delta):\n",
    "   # This function substitutes spikes in data (replaced by nan)\n",
    "   # The finding criteria for a spike is that contiguous values differ by\n",
    "   # more than 3*sigma wrt the distribution of series of differences between consecutive\n",
    "   # values\n",
    "   delta_z_abs=np.abs(stats.zscore(pandas_column.diff(1),nan_policy='omit'))\n",
    "   delta_z_abs[0]=0 # Make first value zero instead of nan\n",
    "   pandas_column_filtered=pandas_column.where(delta_z_abs<sigma_delta, np.nan)\n",
    "   return pandas_column_filtered\n",
    "\n",
    "def reduce_pres(pandas_column,station_height):\n",
    "    #This function reduces pressure in Pa from surface to station height\n",
    "    #It assumes a pressure gradient of 100 Pa each 9m\n",
    "    pandas_column_reduced=pandas_column-100*station_height/9\n",
    "    return pandas_column_reduced\n",
    "\n",
    "\n",
    "def rh2sh(RH, p, T):\n",
    "    \"\"\"Convert relative humidity (0–1), pressure (Pa), and temperature (K) to specific humidity.\"\"\"\n",
    "    RH = RH.to_numpy() * units.dimensionless\n",
    "    p = p.to_numpy() * units.Pa\n",
    "    T = T.to_numpy() * units.kelvin\n",
    "\n",
    "    mixr = mixing_ratio_from_relative_humidity(p, T, RH)\n",
    "    sh = specific_humidity_from_mixing_ratio(mixr)\n",
    "    return sh.magnitude\n",
    "\n",
    "def rh2ah(RH, p, T):\n",
    "    \"\"\"Convert RH (0–1), pressure (Pa), and temperature (K) to absolute humidity (kg/m³).\"\"\"\n",
    "    RH = RH.to_numpy() * units.dimensionless\n",
    "    T = T.to_numpy() * units.kelvin\n",
    "\n",
    "    # Calculate saturation vapor pressure (Pa)\n",
    "    esat = saturation_vapor_pressure(T)\n",
    "\n",
    "    # Vapor pressure (Pa)\n",
    "    e = RH * esat\n",
    "\n",
    "    # Absolute humidity: AH = e / (R_v * T)\n",
    "    R_v = 461.5 * units.joule / (units.kilogram * units.kelvin)\n",
    "    AH = e / (R_v * T)\n",
    "\n",
    "    return AH.magnitude  # returns kg/m³\n",
    "\n",
    "def compute_esat(T_C_array):\n",
    "    \"\"\"Compute saturation vapor pressure from Celsius temperatures (pandas Series or NumPy array).\"\"\"\n",
    "    T_kelvin = (T_C_array.to_numpy() + 273.15) * units.kelvin  # ✅ Use numpy array\n",
    "    esat = saturation_vapor_pressure(T_kelvin).to('Pa').magnitude  # Return float array\n",
    "    return esat\n",
    "\n",
    "#def rh2ah(RH,p,T):\n",
    "#   '''conversion relative humidity to absolute humidity (kg Water per m^3 Air)'''\n",
    "#   mixr=meteo.humidity.rh2mixr(RH, p,T)\n",
    "#   sh=meteo.humidity.mixr2sh(mixr)\n",
    "#   return  sh*meteo.air.rhov(T,p)\n",
    "\n",
    "        \n",
    "def create_lockfile(Forcing_path):\n",
    "    lockfile = os.path.join(Forcing_path, \".lockfile\")\n",
    "    if os.path.exists(lockfile):\n",
    "        raise FileExistsError(f\"Error: A lockfile already exists in {Forcing_path}. Delete it before re-running.\")\n",
    "    with open(lockfile, \"w\") as f:\n",
    "        f.write(\"LOCKED\")\n",
    "\n",
    "################ Write forcing and Params_config file in ascii  ##############################3\n",
    "################ Write forcing and Params_config file in ascii  ##############################3\n",
    "\n",
    "def write_forcing_ascii(Forcing_vars, Forcing_path, Station_forcing, run_start, run_end, \n",
    "                        delta_t, lon, lat, elev, height_T, height_V, write_forcing='yes'):\n",
    "    \"\"\"\n",
    "    Writes forcing variable files and a Params_config.txt metadata file for a simulation run.\n",
    "\n",
    "    Args:\n",
    "        Forcing_vars (list): List of forcing variable names.\n",
    "        Forcing_path (str): Path to save the forcing files.\n",
    "        Station_forcing (DataFrame): Dataframe containing forcing data.\n",
    "        run_start (str): Start time index for data selection.\n",
    "        run_end (str): End time index for data selection.\n",
    "        delta_t (float): Time step (seconds).\n",
    "        lon (float): Longitude of the station.\n",
    "        lat (float): Latitude of the station.\n",
    "        elev (float): Altitude of the station.\n",
    "        height_T (float): Temperature measurement height.\n",
    "        height_V (float): Wind measurement height.\n",
    "        write_forcing (str, optional): Whether to write output files ('yes' to write). Default is 'yes'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"    \n",
    "    os.makedirs(Forcing_path, exist_ok=True)\n",
    "    lockfile = os.path.join(Forcing_path, \".lockfile\")\n",
    "    if write_forcing.lower() == 'yes':\n",
    "        if os.path.exists(lockfile):\n",
    "            raise FileExistsError(f\"Error: A lockfile exists in {Forcing_path}. Delete the lockfile before re-running.\")\n",
    "        create_lockfile(Forcing_path)\n",
    "    \n",
    "    try:\n",
    "        Station_forcing_run = Station_forcing[\n",
    "          (Station_forcing[\"valid_dttm\"] >= run_start) &\n",
    "          (Station_forcing[\"valid_dttm\"] <= run_end)\n",
    "        ]\n",
    "        for var in Forcing_vars:\n",
    "            print(f\"{var} with {Station_forcing_run[var].isna().sum()} NaNs\")\n",
    "            fig, ax = plt.subplots(figsize=(17, 5))\n",
    "            Station_forcing_run[var].plot(ax=ax, label=f\"{var} no_filter\")\n",
    "            ax.set_ylabel(var)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            if write_forcing.lower() == 'yes':\n",
    "                np.savetxt(os.path.join(Forcing_path, f\"{var}.txt\"), \n",
    "                           Station_forcing_run[var].fillna(method='bfill').fillna(method='ffill').values, fmt='%.6f')\n",
    "        \n",
    "        params_lines = [\n",
    "            1, len(Station_forcing_run), delta_t,\n",
    "            Station_forcing_run.valid_dttm[1].year, Station_forcing_run.valid_dttm[1].month,\n",
    "            Station_forcing_run.valid_dttm[1].day, 3600*Station_forcing_run.valid_dttm[1].hour,\n",
    "            lon, lat, elev, height_T, height_V\n",
    "        ]\n",
    "        \n",
    "        if write_forcing.lower() == 'yes':\n",
    "            with open(os.path.join(Forcing_path, \"Params_config.txt\"), 'w') as f:\n",
    "                f.write(\"\\n\".join(map(str, params_lines)) + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "################ Write daily forcing files in netcdf  ##############################\n",
    "        \n",
    "def write_forcing_netcdf(Forcing_vars, Forcing_path, Station_forcing, run_start, run_end,\n",
    "                         delta_t, lon, lat, elev, height_T, height_V, write_forcing='yes'):\n",
    "    os.makedirs(Forcing_path, exist_ok=True)\n",
    "    \n",
    "    existing_files = set()\n",
    "    time_range = pd.date_range(run_start, run_end, freq=\"D\")\n",
    "    \n",
    "    for date in time_range:\n",
    "        forcing_filename = os.path.join(Forcing_path, f\"FORCING.nc_{date.strftime('%Y%m%d')}\")\n",
    "        if os.path.exists(forcing_filename):\n",
    "            existing_files.add(forcing_filename)\n",
    "    \n",
    "    if write_forcing.lower() == 'yes' and existing_files:\n",
    "        raise FileExistsError(f\"Error: Existing forcing files would be overwritten: {existing_files}. Delete them before re-running.\")\n",
    "    \n",
    "    try:\n",
    "        Station_forcing_run = Station_forcing[\n",
    "          (Station_forcing[\"valid_dttm\"] >= run_start) &\n",
    "          (Station_forcing[\"valid_dttm\"] <= run_end)\n",
    "        ]\n",
    "        forcing_vars = [\"CO2air\", \"Wind_DIR\", \"PSurf\", \"Rainf\", \"Snowf\", \"Wind\", \"DIR_SWdown\",\n",
    "                        \"LWdown\", \"Qair\", \"SCA_SWdown\", \"Tair\"]\n",
    "        forcing_var_map = dict(zip(Forcing_vars, forcing_vars))\n",
    "\n",
    "        for forcing_key in Forcing_vars:\n",
    "            fig, ax = plt.subplots(figsize=(17, 5))\n",
    "            Station_forcing_run[forcing_key].plot(ax=ax, label=f\"{forcing_key} no_filter\")\n",
    "            ax.set_ylabel(forcing_key)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "        for date in time_range:\n",
    "            daily_data = Station_forcing_run[\n",
    "                  Station_forcing_run[\"valid_dttm\"].dt.date == date.date()\n",
    "            ] \n",
    "            \n",
    "            time_values = pd.date_range(date, periods=daily_data.shape[0], freq=\"h\").to_pydatetime()\n",
    "            forcing_filename = os.path.join(Forcing_path, f\"FORCING.nc_{date.strftime('%Y%m%d')}\")\n",
    "            with Dataset(forcing_filename, \"w\", format=\"NETCDF4\") as nc:\n",
    "                nc.createDimension(\"time\", len(time_values))\n",
    "                nc.createDimension(\"Number_of_points\", 1)\n",
    "                \n",
    "                time_var = nc.createVariable(\"time\", \"f4\", (\"time\",))\n",
    "                time_var.units = \"seconds since 2014-01-01 00:00:00\"\n",
    "                time_var[:] = date2num(time_values, units=time_var.units, calendar='gregorian')\n",
    "                \n",
    "                meta_vars = {\n",
    "                    \"FRC_TIME_STP\": (\"Forcing_Time_Step\", delta_t, None),\n",
    "                    \"LON\": (\"Longitude\", lon, \"Number_of_points\"),\n",
    "                    \"LAT\": (\"Latitude\", lat, \"Number_of_points\"),\n",
    "                    \"ZS\": (\"Surface_Orography\", elev, \"Number_of_points\"),\n",
    "                    \"ZREF\": (\"Reference_Height\", height_T, \"Number_of_points\"),\n",
    "                    \"UREF\": (\"Reference_Height_for_Wind\", height_V, \"Number_of_points\")\n",
    "                }\n",
    "                \n",
    "                for var_name, (long_name, value, dim) in meta_vars.items():\n",
    "                    var = nc.createVariable(var_name, \"f4\", (dim,) if dim else ())\n",
    "                    var.long_name = long_name\n",
    "                    if var_name in [\"ZREF\", \"UREF\"]:\n",
    "                        var.units = \"m\"\n",
    "                    var[...] = value\n",
    "                \n",
    "                forcing_data_vars = {}\n",
    "                for var in forcing_vars:\n",
    "                    forcing_data_vars[var] = nc.createVariable(var, \"f4\", (\"time\", \"Number_of_points\"))\n",
    "\n",
    "                attr_map = {\n",
    "                    \"Tair\": (\"Near_Surface_Air_Temperature\", \"2m\", \"K\"),\n",
    "                    \"Qair\": (\"Near_Surface_Specific_Humidity\", \"2m\", \"Kg/Kg\"),\n",
    "                    \"PSurf\": (\"Surface_Pressure\", \"2m\", \"Pa\"),\n",
    "                    \"DIR_SWdown\": (\"Surface_Indicent_Direct_Shortwave_Radiation\", None, \"W/m2\"),\n",
    "                    \"SCA_SWdown\": (\"Surface_Incident_Diffuse_Shortwave_Radiation\", None, \"W/m2\"),\n",
    "                    \"LWdown\": (\"Surface_Incident_Longwave_Radiation\", None, \"W/m2\"),\n",
    "                    \"Rainf\": (\"Rainfall_Rate\", None, \"Kg/m2/s\"),\n",
    "                    \"Snowf\": (\"Snowfall_Rate\", None, \"Kg/m2/s\"),\n",
    "                    \"Wind\": (\"Wind_Speed\", None, \"m/s\"),\n",
    "                    \"Wind_DIR\": (\"Wind_Direction\", None, \"deg\"),\n",
    "                    \"CO2air\": (\"Near_Surface_CO2_Concentration\", None, \"Kg/m3\")\n",
    "                }\n",
    "                \n",
    "                for var_name, (long_name, height, unit) in attr_map.items():\n",
    "                    var = forcing_data_vars[var_name]\n",
    "                    var.long_name = long_name\n",
    "                    if height:\n",
    "                        var.measurement_height = height\n",
    "                    var.units = unit\n",
    "                \n",
    "                if write_forcing.lower() == 'yes':\n",
    "                    for forcing_key, nc_var in forcing_var_map.items():\n",
    "                        forcing_data_vars[nc_var][:] = daily_data[forcing_key].values.reshape(len(time_values), 1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered: {e}\")\n",
    "        raise\n",
    "\n",
    "################ Merge netcdf files for a period which is already downloaded as daily files #############\n",
    "\n",
    "def merge_forcing_netcdf(Forcing_path, start_date, end_date, output_filename=\"FORCING.nc\"):\n",
    "    \"\"\"\n",
    "    Merges daily NetCDF forcing files into a single file with the correct structure.\n",
    "    \"\"\"\n",
    "    merged_filepath = os.path.join(Forcing_path, output_filename)\n",
    "    \n",
    "    # Generate list of daily forcing files to merge\n",
    "    time_range = pd.date_range(start_date, end_date, freq=\"D\")\n",
    "    forcing_files = [os.path.join(Forcing_path, f\"FORCING.nc_{date.strftime('%Y%m%d')}\") for date in time_range]\n",
    "    forcing_files = [f for f in forcing_files if os.path.exists(f)]\n",
    "    \n",
    "    if not forcing_files:\n",
    "        raise FileNotFoundError(\"No forcing files found for the specified date range.\")\n",
    "    \n",
    "    # Read first file to get metadata\n",
    "    with Dataset(forcing_files[0], \"r\") as sample_nc:\n",
    "        forcing_vars = [var for var in sample_nc.variables if var not in [\"time\", \"Number_of_points\", \"FRC_TIME_STP\", \"LON\", \"LAT\", \"ZS\", \"ZREF\", \"UREF\"]]\n",
    "        static_vars = [var for var in sample_nc.variables if var in [\"FRC_TIME_STP\", \"LON\", \"LAT\", \"ZS\", \"ZREF\", \"UREF\"]]\n",
    "    \n",
    "    # Initialize storage\n",
    "    merged_data = {var: [] for var in forcing_vars}\n",
    "    merged_time = []\n",
    "    static_data = {}\n",
    "    \n",
    "    for file in forcing_files:\n",
    "        with Dataset(file, \"r\") as nc:\n",
    "            merged_time.extend(nc.variables[\"time\"][...])\n",
    "            for var in forcing_vars:\n",
    "                merged_data[var].append(nc.variables[var][...])\n",
    "            if not static_data:\n",
    "                for var in static_vars:\n",
    "                    static_data[var] = nc.variables[var][...]\n",
    "    \n",
    "    # Concatenate data along the time dimension\n",
    "    for var in forcing_vars:\n",
    "        print(f\"Variable: {var}, Data: {merged_data[var]}\")\n",
    "        merged_data[var] = np.concatenate(merged_data[var], axis=0)\n",
    "    \n",
    "    # Write merged data to new NetCDF file\n",
    "    with Dataset(merged_filepath, \"w\", format=\"NETCDF4\") as nc:\n",
    "        nc.createDimension(\"time\", len(merged_time))\n",
    "        nc.createDimension(\"Number_of_points\", 1)\n",
    "        \n",
    "        # Create time variable\n",
    "        time_var = nc.createVariable(\"time\", \"f8\", (\"time\",))\n",
    "        time_var.units = \"seconds since 2014-01-01 01:00:00\"\n",
    "        time_var[:] = merged_time\n",
    "        \n",
    "        # Create static variables\n",
    "        for var, data in static_data.items():\n",
    "            nc_var = nc.createVariable(var, \"f4\", (\"Number_of_points\",))\n",
    "            nc_var.long_name = var.replace(\"_\", \" \")\n",
    "            nc_var[:] = data\n",
    "        \n",
    "        # Create forcing variables\n",
    "        for var in forcing_vars:\n",
    "            nc_var = nc.createVariable(var, \"f4\", (\"time\", \"Number_of_points\"))\n",
    "            nc_var.long_name = var.replace(\"_\", \" \")\n",
    "            nc_var[:] = merged_data[var]\n",
    "    \n",
    "    print(f\"Merged NetCDF file created: {merged_filepath}\")\n",
    "\n",
    "def process_data(df, variable_map, station_info, start, end, transform_map=None, timedelta_minutes=None):\n",
    "    df['valid_dttm'] = pd.to_datetime(df['TIMESTAMP'], utc=True)\n",
    "    df = df[(df['valid_dttm'] >= start) & (df['valid_dttm'] <= end)].copy()\n",
    "\n",
    "    source_vars = list(variable_map.values())\n",
    "    df = df.dropna(subset=source_vars)\n",
    "\n",
    "    df['SID'] = station_info['SID']\n",
    "    df['lat'] = station_info['lat']\n",
    "    df['lon'] = station_info['lon']\n",
    "    df['elev'] = station_info['elev']\n",
    "\n",
    "    # Rename columns\n",
    "    df = df.rename(columns={v: k for k, v in variable_map.items()})\n",
    "\n",
    "    # Apply transformations\n",
    "    if transform_map:\n",
    "        for target_var, expr in transform_map.items():\n",
    "            # Replace \"timedelta\" in expression with numeric value (in minutes)\n",
    "            safe_expr = expr.replace(\"timedelta\", str(timedelta_minutes))\n",
    "            try:\n",
    "                df[target_var] = df.eval(f\"{target_var} {safe_expr}\")\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Error applying transformation for {target_var}: {expr}\\n{e}\")\n",
    "\n",
    "    selected_columns = ['valid_dttm', 'SID', 'lat', 'lon', 'elev'] + list(variable_map.keys())\n",
    "    return df[selected_columns]\n",
    "\n",
    "\n",
    "def fetch_flux_data(doi):\n",
    "    dobj = Dobj(doi)\n",
    "    df = dobj.data\n",
    "    return df\n",
    "\n",
    "def upsample_to_common_timedelta(datasets, dfs, common_td):\n",
    "    dfs_resampled = []\n",
    "\n",
    "    for name, df in zip(datasets.keys(), dfs):\n",
    "        orig_td = pd.to_timedelta(datasets[name][\"timedelta\"], unit=\"m\")\n",
    "        if orig_td == common_td:\n",
    "            dfs_resampled.append(df)\n",
    "        else:\n",
    "            df = df.set_index(\"valid_dttm\")\n",
    "            df = df.resample(common_td).interpolate(method=\"linear\")\n",
    "            df = df.reset_index()\n",
    "            dfs_resampled.append(df)\n",
    "\n",
    "    return dfs_resampled\n",
    "\n",
    "def parse_variable_entry(entry_str, timedelta_minutes):\n",
    "    if entry_str is None or entry_str.strip() == \"\":\n",
    "        return None, \"const\", 0.0\n",
    "\n",
    "    parts = [p.strip() for p in entry_str.split(\",\", maxsplit=1)]\n",
    "\n",
    "    # Case: constant only\n",
    "    if len(parts) == 1:\n",
    "        val = parts[0]\n",
    "        if val in [\"-\", \"None\", \"\"]:\n",
    "            return None, \"zero\", 0.0\n",
    "        try:\n",
    "            return None, \"const\", float(val)\n",
    "        except ValueError:\n",
    "            return val, None, None  # just a direct mapping\n",
    "\n",
    "    # Case: transformation\n",
    "    src, transform = parts\n",
    "    if src in [\"-\", \"None\", \"\"]:\n",
    "        try:\n",
    "            return None, \"const\", float(transform)\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"Invalid constant value in entry: {entry_str}\")\n",
    "\n",
    "    if \"timedelta\" in transform:\n",
    "        transform = transform.replace(\"timedelta\", str(timedelta_minutes))\n",
    "\n",
    "    op = transform[0]\n",
    "    expr = transform[1:].strip()\n",
    "\n",
    "    try:\n",
    "        val = eval(expr, {}, {})  # safe eval of math expression\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to evaluate expression '{expr}' in entry '{entry_str}': {e}\")\n",
    "\n",
    "    return src, op, val\n",
    "\n",
    "\n",
    "\n",
    "def apply_transformation(series, op, val):\n",
    "    if op == \"+\":\n",
    "        return series + val\n",
    "    elif op == \"-\":\n",
    "        return series - val\n",
    "    elif op == \"*\":\n",
    "        return series * val\n",
    "    elif op == \"/\":\n",
    "        return series / val\n",
    "    else:\n",
    "        return series  # No op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2589587",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### OSVAS #####################################################\n",
    "###### ( OFFLINE SURFEX VALIDATION SYSTEM)########################\n",
    "#### STEP 2: LOAD STATION METADATA AND CONFIGURATION OF  #########\n",
    "#### THE FORCING GENERATION FROM THE STATION'S YAML FILE #########\n",
    "\n",
    "#3.1 Read YAML config, define paths, set Station,\n",
    "\n",
    "OSVAS='/home/pn56/OSVASgh/'  # Main OSVAS path\n",
    "home = os.path.join(str(Path.home()),OSVAS)  \n",
    "Station_name='Loobos'\n",
    "os.chdir(OSVAS)\n",
    "write_forcing='yes' #Set to yes to write forcing\n",
    "CONFIG_PATH = f\"config_files/Stations/{Station_name}.yml\"\n",
    "\n",
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "station_info = config[\"Station_metadata\"]\n",
    "forcing_data = config[\"Forcing_data\"]\n",
    "lon=config[\"Station_metadata\"][\"lon\"]\n",
    "lat=config[\"Station_metadata\"][\"lat\"]\n",
    "elev=config[\"Station_metadata\"][\"elev\"]\n",
    "height_T=forcing_data[\"height_T\"]\n",
    "height_V=forcing_data[\"height_V\"]\n",
    "forcing_format = forcing_data[\"forcing_format\"]\n",
    "\n",
    "\n",
    "start_date = pd.to_datetime(forcing_data[\"run_start\"], utc=True)\n",
    "end_date = pd.to_datetime(forcing_data[\"run_end\"], utc=True)\n",
    "\n",
    "# Surfex vegetation types : \n",
    "#1: no vegetation (smooth) - NO    2: no vegetation (rocks) - ROCK    3: permanent snow and ice - SNOW\n",
    "#4: temperate broadleaf cold-deciduous summergreen - TEBD    5: boreal needleleaf evergreen - BONE\n",
    "#6: tropical broadleaf evergreen - EVER    7: C3 cultures types - C3    8: C4 cultures types - C4\n",
    "#9: irrigated crops - IRR10: grassland (C3) - GRAS     11: tropical grassland (C4) - TROG\n",
    "#12: peat bogs, parks and gardens (irrigated grass) - PARK     13: tropical broadleaf deciduous - TRBD\n",
    "#14: temperate broadleaf evergreen - TEBE     15: temperate needleleaf evergreen - TENE\n",
    "#16: boreal broadleaf cold-deciduous summergreen - BOBD    17: boreal needleleaf cold-deciduous summergreen - BOND\n",
    "#18: boreal grass - BOGR    19: shrub - SHRB\n",
    "\n",
    "###### OSVAS ###################################\n",
    "###### ( OFFLINE SURFEX VALIDATION SYSTEM)######\n",
    "#### STEP 3: ICOS AUTHENTICATION ###############\n",
    "\n",
    "# More info : https://icos-carbon-portal.github.io/pylib//icoscp/authentication/\n",
    "\n",
    "# There are several ways to authenticate yourself into ICOS\n",
    "# In the example below, the temporal API token is used, which is available \n",
    "# at the bottom of https://cpauth.icos-cp.eu/home/ after you authenticate\n",
    "# into the portal. The token lasts for 100.000 seconds, ~28 hours.\n",
    "\n",
    "# Authenticate using cookie (adjust if needed)\n",
    "cookie_path = \"icos_cookie.txt\"  # Or point to config\n",
    "cookie_token = open(cookie_path, \"r\").readline().strip()\n",
    "meta, data = bootstrap.fromCookieToken(cookie_token)\n",
    "cpauth.init_by(data.auth)\n",
    "\n",
    "#Test: If the authentication went well, these lines of code will not fail:\n",
    "import icoscp\n",
    "from icoscp.dobj import Dobj\n",
    "obj_flux='https://meta.icos-cp.eu/objects/dDlpnhS3XKyZjB22MUzP_nAm'\n",
    "dobj_flux=Dobj(obj_flux).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13989dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### OSVAS #####################################################\n",
    "###### ( OFFLINE SURFEX VALIDATION SYSTEM)########################\n",
    "#### STEP 4: LOAD STATION DATA, READ VARIABLES ###############\n",
    "#### TRANSFORM TO UNITS USED BY SURFEX ######################################\n",
    "\n",
    "\n",
    "#Read all datasets from the Forcing_data section of the yaml file, loop over datasets,\n",
    "#loop over datasets to get all forcing variables, abort if no data in range\n",
    "dfs = []\n",
    "timedeltas = []\n",
    "datasets = {k: v for k, v in forcing_data.items() if k.startswith(\"dataset\") or k.startswith(\"dataset_\")}\n",
    "\n",
    "for ds_name, ds_info in datasets.items():\n",
    "    print(f\"Processing {ds_name} from DOI: {ds_info['doi']}\")\n",
    "    doi = ds_info[\"doi\"]\n",
    "    timedelta_minutes = ds_info[\"timedelta\"]\n",
    "    timedeltas.append(timedelta_minutes)\n",
    "\n",
    "    variable_map_raw = ds_info[\"variables\"]\n",
    "\n",
    "    df_raw = fetch_flux_data(doi)\n",
    "    df_raw['valid_dttm'] = pd.to_datetime(df_raw['TIMESTAMP'], utc=True)\n",
    "    df_raw = df_raw[(df_raw['valid_dttm'] >= start_date) & (df_raw['valid_dttm'] <= end_date)].copy()\n",
    "\n",
    "    if df_raw.empty:\n",
    "        raise RuntimeError(\n",
    "            f\"❌ No data found in the time window ({start_date} to {end_date}) \"\n",
    "            f\"for dataset {ds_name} (DOI: {doi}). Aborting.\"\n",
    "        )\n",
    "\n",
    "    df_processed = pd.DataFrame()\n",
    "    df_processed['valid_dttm'] = df_raw['valid_dttm'].copy()\n",
    "    df_processed['SID'] = station_info['SID']\n",
    "    df_processed['lat'] = station_info['lat']\n",
    "    df_processed['lon'] = station_info['lon']\n",
    "    df_processed['elev'] = station_info['elev']\n",
    "\n",
    "    for target_var, entry in variable_map_raw.items():\n",
    "        src, op, val = parse_variable_entry(entry, timedelta_minutes)\n",
    "    \n",
    "        if src is None:\n",
    "            if op == \"zero\":\n",
    "                df_processed[target_var] = 0.0\n",
    "            elif op == \"const\":\n",
    "                df_processed[target_var] = val\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown operation {op} for {target_var}\")\n",
    "        else:\n",
    "            if op is None:\n",
    "                df_processed[target_var] = df_raw[src].values\n",
    "            elif op == \"+\":\n",
    "                df_processed[target_var] = df_raw[src] + val\n",
    "            elif op == \"-\":\n",
    "                df_processed[target_var] = df_raw[src] - val\n",
    "            elif op == \"*\":\n",
    "                df_processed[target_var] = df_raw[src] * val\n",
    "            elif op == \"/\":\n",
    "                df_processed[target_var] = df_raw[src] / val\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown transformation operator {op}\")\n",
    "    dfs.append(df_processed)\n",
    "\n",
    "#  Harmonize resolution\n",
    "common_td = pd.to_timedelta(min(timedeltas), unit=\"m\")  # Choose finest resolution\n",
    "dfs_resampled = upsample_to_common_timedelta(datasets, dfs, common_td)\n",
    "\n",
    "#  Merge all datasets\n",
    "from functools import reduce\n",
    "Station_forcing = reduce(lambda left, right: pd.merge(left, right, on=['valid_dttm', 'SID', 'lat', 'lon', 'elev'], how='outer'), dfs_resampled)\n",
    "Station_forcing = Station_forcing.sort_values(\"valid_dttm\").reset_index(drop=True)\n",
    "\n",
    "#  Compute Forc_QA from ICOS RH or VPD variables if needed:\n",
    "\n",
    "# Step 1: If Forc_RH doesn't exist, compute it from Forc_VPD (if available)\n",
    "if \"Forc_RH\" not in Station_forcing.columns and \"Forc_VPD\" in Station_forcing.columns:\n",
    "    esat = compute_esat(Station_forcing[\"Forc_TA\"])\n",
    "    df[\"Forc_RH\"] = 100 * (esat - Station_forcing[\"Forc_VPD\"]) / esat\n",
    "\n",
    "# Step 2: If Forc_QA is full of zeros, compute it from Forc_RH, Forc_PS, and Forc_TA\n",
    "if \"Forc_QA\" in Station_forcing.columns and np.all(Station_forcing[\"Forc_QA\"] == 0):\n",
    "    required_cols = [\"Forc_RH\", \"Forc_PS\", \"Forc_TA\"]\n",
    "    if all(col in Station_forcing.columns for col in required_cols):\n",
    "        # Create a valid mask for non-NaN and physically reasonable values\n",
    "        valid_mask = (\n",
    "            Station_forcing[\"Forc_RH\"].between(0, 100) &\n",
    "            Station_forcing[\"Forc_PS\"].between(1e3, 1.2e5) &  # Pressure between 1kPa and 120kPa\n",
    "            Station_forcing[\"Forc_TA\"].between(150, 330) &    # Temp between -123°C and 57°C\n",
    "            Station_forcing[[\"Forc_RH\", \"Forc_PS\", \"Forc_TA\"]].notnull().all(axis=1)\n",
    "        )\n",
    "\n",
    "        if valid_mask.any():\n",
    "            Station_forcing.loc[valid_mask, \"Forc_QA\"] = rh2sh(\n",
    "                Station_forcing.loc[valid_mask, \"Forc_RH\"] / 100,\n",
    "                Station_forcing.loc[valid_mask, \"Forc_PS\"],\n",
    "                Station_forcing.loc[valid_mask, \"Forc_TA\"]\n",
    "            )\n",
    "        else:\n",
    "            print(\"No valid rows to compute Forc_QA.\")\n",
    "    else:\n",
    "        print(\"Cannot compute Forc_QA: missing Forc_RH, Forc_PS, or Forc_TA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d932f7d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###### OSVAS ####################################################\n",
    "###### ( OFFLINE SURFEX VALIDATION SYSTEM)#######################\n",
    "#### STEP 5: PLOT FORCING VARIABLES FOR THE SELECTED PERIOD #####\n",
    "#### WRITE THE FORCING FILES IN THE SELECTED FILE TYPE ##########\n",
    "\n",
    "\n",
    "Forcing_vars=['Forc_CO2','Forc_DIR','Forc_PS','Forc_RAIN','Forc_SNOW','Forc_WIND','Forc_DIR_SW','Forc_LW','Forc_QA','Forc_SCA_SW','Forc_TA']\n",
    "Forcing_path=os.path.join(home,'forcings',Station_name)\n",
    "write_forcing='yes' #Set to yes for writing forcing\n",
    "\n",
    "if forcing_format=='ascii':\n",
    "    write_forcing_ascii(\n",
    "    Forcing_vars=Forcing_vars, \n",
    "    Forcing_path=Forcing_path,\n",
    "    Station_forcing=Station_forcing, \n",
    "    run_start=start_date, \n",
    "    run_end=end_date, \n",
    "    delta_t=common_td.seconds, \n",
    "    lon=lon, lat=lat, elev=elev, \n",
    "    height_T=height_T, height_V=height_V,\n",
    "    write_forcing=write_forcing\n",
    ")\n",
    "    \n",
    "if forcing_format=='netcdf':\n",
    "    write_forcing_netcdf(\n",
    "    Forcing_vars=Forcing_vars, \n",
    "    Forcing_path=Forcing_path,\n",
    "    Station_forcing=Station_forcing, \n",
    "    run_start=start_date, \n",
    "    run_end=end_date, \n",
    "    delta_t=common_td.seconds, \n",
    "    lon=lon, lat=lat, elev=elev, \n",
    "    height_T=height_T, height_V=height_V,\n",
    "    write_forcing=write_forcing\n",
    ")\n",
    "\n",
    "\n",
    "#### Example on how to reconstruct a forcing file for a period from the individual netcdf files\n",
    "#run_start='2016-5-1 00:00:00'    # Timestamp for the forcing start\n",
    "#run_end='2016-7-1 00:00:00'  \n",
    "#run_start='2018-6-1 00:00:00'    # Timestamp for the forcing start\n",
    "#run_end='2022-6-6 00:00:00' \n",
    "#merge_forcing_netcdf(Forcing_path=Forcing_path,start_date=run_start, end_date=run_end, output_filename=\"FORCING.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438af085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe8ba9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8521e43b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efc96fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
