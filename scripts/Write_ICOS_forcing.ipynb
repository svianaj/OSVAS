{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed8a706",
   "metadata": {},
   "outputs": [],
   "source": [
    "    ###### OSVAS ###################################\n",
    "    ###### ( OFFLINE SURFEX VALIDATION SYSTEM)######\n",
    "    #### STEP 1: ICOS AUTHENTICATION ###############\n",
    "\n",
    "#A username/password account for the ICOS authentication service is required for this.\n",
    "#Obfuscated (not readable by humans) password is stored in a file on the local machine in a default user-specific\n",
    "#folder. To initialize this file, run the following code interactively \n",
    "#(only needs to be done once for every machine):\n",
    "\n",
    "from icoscp_core.icos import auth\n",
    "#auth.init_config_file()\n",
    "#obj_flux='https://meta.icos-cp.eu/objects/dDlpnhS3XKyZjB22MUzP_nAm'\n",
    "\n",
    "#dobj_flux=Dobj(obj_flux).data\n",
    "#nBTx3mW2Y2wdiHD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9ed2366",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### OSVAS ########################################################\n",
    "###### ( OFFLINE SURFEX VALIDATION SYSTEM)###########################\n",
    "#### STEP 2: IMPORTING NEEDED PACKAGES AND DEFINING FUNCTIONS ###############\n",
    "import icoscp\n",
    "from icoscp.dobj import Dobj\n",
    "import sys\n",
    "sys.path.append(\"~/.local/lib/python3.10/site-packages/\")\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "from datetime import date, datetime, timedelta\n",
    "import os\n",
    "import time\n",
    "import meteo\n",
    "# Note: meteo package available here: https://github.com/hendrikwout/meteo\n",
    "from pathlib import Path\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "from netCDF4 import Dataset, date2num\n",
    "import cftime\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 500\n",
    "##############################################################################\n",
    "##Here comes a series of functions for handling the forcing creation easily ##\n",
    "##############################################################################\n",
    "def datespan(startDate, endDate, delta=timedelta(days=1)):\n",
    "    currentDate = startDate\n",
    "    while currentDate < endDate:\n",
    "        yield currentDate\n",
    "        currentDate += delta\n",
    "\n",
    "def despike(pandas_column, sigma_delta):\n",
    "   # This function substitutes spikes in data (replaced by nan)\n",
    "   # The finding criteria for a spike is that contiguous values differ by\n",
    "   # more than 3*sigma wrt the distribution of series of differences between consecutive\n",
    "   # values\n",
    "   delta_z_abs=np.abs(stats.zscore(pandas_column.diff(1),nan_policy='omit'))\n",
    "   delta_z_abs[0]=0 # Make first value zero instead of nan\n",
    "   pandas_column_filtered=pandas_column.where(delta_z_abs<sigma_delta, np.nan)\n",
    "   return pandas_column_filtered\n",
    "\n",
    "def reduce_pres(pandas_column,station_height):\n",
    "    #This function reduces pressure in Pa from surface to station height\n",
    "    #It assumes a pressure gradient of 100 Pa each 9m\n",
    "    pandas_column_reduced=pandas_column-100*station_height/9\n",
    "    return pandas_column_reduced\n",
    "\n",
    "def rh2ah(RH,p,T):\n",
    "   '''conversion relative humidity to absolute humidity (kg Water per m^3 Air)'''\n",
    "   mixr=meteo.humidity.rh2mixr(RH, p,T)\n",
    "   sh=meteo.humidity.mixr2sh(mixr)\n",
    "   return  sh*meteo.air.rhov(T,p)\n",
    "\n",
    "        \n",
    "def create_lockfile(Forcing_path):\n",
    "    lockfile = os.path.join(Forcing_path, \".lockfile\")\n",
    "    if os.path.exists(lockfile):\n",
    "        raise FileExistsError(f\"Error: A lockfile already exists in {Forcing_path}. Delete it before re-running.\")\n",
    "    with open(lockfile, \"w\") as f:\n",
    "        f.write(\"LOCKED\")\n",
    "\n",
    "################ Write forcing and Params_config file in ascii  ##############################3\n",
    "\n",
    "def write_forcing_ascii(Forcing_vars, Forcing_path, Station_forcing, run_start, run_end, \n",
    "                        delta_t, lon, lat, alt, height_T, height_V, write_forcing_run='yes'):\n",
    "    \"\"\"\n",
    "    Writes forcing variable files and a Params_config.txt metadata file for a simulation run.\n",
    "\n",
    "    Args:\n",
    "        Forcing_vars (list): List of forcing variable names.\n",
    "        Forcing_path (str): Path to save the forcing files.\n",
    "        Station_forcing (DataFrame): Dataframe containing forcing data.\n",
    "        run_start (str): Start time index for data selection.\n",
    "        run_end (str): End time index for data selection.\n",
    "        delta_t (float): Time step (seconds).\n",
    "        lon (float): Longitude of the station.\n",
    "        lat (float): Latitude of the station.\n",
    "        alt (float): Altitude of the station.\n",
    "        height_T (float): Temperature measurement height.\n",
    "        height_V (float): Wind measurement height.\n",
    "        write_forcing_run (str, optional): Whether to write output files ('yes' to write). Default is 'yes'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"    \n",
    "    os.makedirs(Forcing_path, exist_ok=True)\n",
    "    lockfile = os.path.join(Forcing_path, \".lockfile\")\n",
    "    if write_forcing_run.lower() == 'yes':\n",
    "        if os.path.exists(lockfile):\n",
    "            raise FileExistsError(f\"Error: A lockfile exists in {Forcing_path}. Delete the lockfile before re-running.\")\n",
    "        create_lockfile(Forcing_path)\n",
    "    \n",
    "    try:\n",
    "        Station_forcing_run = Station_forcing.loc[run_start:run_end]\n",
    "        for var in Forcing_vars:\n",
    "            print(f\"{var} with {Station_forcing_run[var].isna().sum()} NaNs\")\n",
    "            fig, ax = plt.subplots(figsize=(17, 5))\n",
    "            Station_forcing_run[var].plot(ax=ax, label=f\"{var} no_filter\")\n",
    "            ax.set_ylabel(var)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            if write_forcing_run.lower() == 'yes':\n",
    "                np.savetxt(os.path.join(Forcing_path, f\"{var}.txt\"), \n",
    "                           Station_forcing_run[var].fillna(method='bfill').fillna(method='ffill').values, fmt='%.6f')\n",
    "        \n",
    "        params_lines = [\n",
    "            1, len(Station_forcing_run), delta_t,\n",
    "            Station_forcing_run.index[1].year, Station_forcing_run.index[1].month,\n",
    "            Station_forcing_run.index[1].day, Station_forcing_run.index[1].hour,\n",
    "            lon, lat, alt, height_T, height_V\n",
    "        ]\n",
    "        \n",
    "        if write_forcing_run.lower() == 'yes':\n",
    "            with open(os.path.join(Forcing_path, \"Params_config.txt\"), 'w') as f:\n",
    "                f.write(\"\\n\".join(map(str, params_lines)) + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered: {e}\")\n",
    "        raise\n",
    "\n",
    "################ Write daily forcing files in netcdf  ##############################\n",
    "        \n",
    "def write_forcing_netcdf(Forcing_vars, Forcing_path, Station_forcing, run_start, run_end,\n",
    "                         delta_t, lon, lat, alt, height_T, height_V, write_forcing_run='yes'):\n",
    "    \"\"\"\n",
    "    Writes forcing variable file in netcdf.\n",
    "\n",
    "    Args:\n",
    "        Forcing_vars (list): List of forcing variable names.\n",
    "        Forcing_path (str): Path to save the forcing files.\n",
    "        Station_forcing (DataFrame): Dataframe containing forcing data.\n",
    "        run_start (str): Start time index for data selection.\n",
    "        run_end (str): End time index for data selection.\n",
    "        delta_t (float): Time step (seconds).\n",
    "        lon (float): Longitude of the station.\n",
    "        lat (float): Latitude of the station.\n",
    "        alt (float): Altitude of the station.\n",
    "        height_T (float): Temperature measurement height.\n",
    "        height_V (float): Wind measurement height.\n",
    "        write_forcing_run (str, optional): Whether to write output files ('yes' to write). Default is 'yes'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    os.makedirs(Forcing_path, exist_ok=True)\n",
    "    \n",
    "    existing_files = set()\n",
    "    time_range = pd.date_range(run_start, run_end, freq=\"D\")\n",
    "    \n",
    "    for date in time_range:\n",
    "        forcing_filename = os.path.join(Forcing_path, f\"FORCING.nc_{date.strftime('%Y%m%d')}\")\n",
    "        if os.path.exists(forcing_filename):\n",
    "            existing_files.add(forcing_filename)\n",
    "    \n",
    "    if write_forcing_run.lower() == 'yes' and existing_files:\n",
    "        raise FileExistsError(f\"Error: Existing forcing files would be overwritten: {existing_files}. Delete them before re-running.\")\n",
    "    \n",
    "    try:\n",
    "        Station_forcing_run = Station_forcing.loc[run_start:run_end]\n",
    "        forcing_vars = [\"CO2air\", \"Wind_DIR\", \"PSurf\", \"Rainf\", \"Snowf\", \"Wind\", \"DIR_SWdown\",\n",
    "                        \"LWdown\", \"Qair\", \"SCA_SWdown\", \"Tair\"]\n",
    "        forcing_var_map = dict(zip(Forcing_vars, forcing_vars))\n",
    "        \n",
    "        # Plot full period\n",
    "        for forcing_key in Forcing_vars:\n",
    "            fig, ax = plt.subplots(figsize=(17, 5))\n",
    "            Station_forcing_run[forcing_key].plot(ax=ax, label=f\"{forcing_key} no_filter\")\n",
    "            ax.set_ylabel(forcing_key)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        \n",
    "        for date in time_range:\n",
    "            daily_data = Station_forcing_run.loc[date.strftime('%Y-%m-%d')]\n",
    "            time_values = pd.date_range(date, periods=daily_data.shape[0], freq=\"h\").to_pydatetime()\n",
    "            \n",
    "            forcing_filename = os.path.join(Forcing_path, f\"FORCING.nc_{date.strftime('%Y%m%d')}\")\n",
    "            with Dataset(forcing_filename, \"w\", format=\"NETCDF4\") as nc:\n",
    "                nc.createDimension(\"time\", len(time_values))\n",
    "                nc.createDimension(\"Number_of_points\", 1)\n",
    "                \n",
    "                time_var = nc.createVariable(\"time\", \"f8\", (\"time\",))\n",
    "                time_var.units = \"seconds since 2014-01-01 01:00:00\"\n",
    "                time_var[:] = date2num(time_values, units=time_var.units, calendar='gregorian')\n",
    "                \n",
    "                forcing_data_vars = {var: nc.createVariable(var, \"f4\", (\"time\", \"Number_of_points\")) for var in forcing_vars}\n",
    "                \n",
    "                if write_forcing_run.lower() == 'yes':\n",
    "                    for forcing_key, nc_var in forcing_var_map.items():\n",
    "                        forcing_data_vars[nc_var][:] = daily_data[forcing_key].values.reshape(len(time_values), 1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered: {e}\")\n",
    "        raise\n",
    "        \n",
    "        \n",
    "################ Merge netcdf files for a period which is already downloaded as daily files #############\n",
    "\n",
    "def merge_forcing_netcdf(Forcing_path, start_date, end_date, output_filename=\"FORCING.nc\"):\n",
    "    \"\"\"\n",
    "    Merges daily NetCDF forcing files into a single file.\n",
    "\n",
    "    Args:\n",
    "        Forcing_path (str): Path where the daily NetCDF files are stored.\n",
    "        start_date (str): Start date in 'YYYY-MM-DD' format.\n",
    "        end_date (str): End date in 'YYYY-MM-DD' format.\n",
    "        output_filename (str, optional): Name of the merged NetCDF file. Default is 'FORCING.nc'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    merged_filepath = os.path.join(Forcing_path, output_filename)\n",
    "    \n",
    "    # Generate list of daily forcing files to merge\n",
    "    time_range = pd.date_range(start_date, end_date, freq=\"D\")\n",
    "    forcing_files = [os.path.join(Forcing_path, f\"FORCING.nc_{date.strftime('%Y%m%d')}\") for date in time_range]\n",
    "    forcing_files = [f for f in forcing_files if os.path.exists(f)]\n",
    "    \n",
    "    if not forcing_files:\n",
    "        raise FileNotFoundError(\"No forcing files found for the specified date range.\")\n",
    "    \n",
    "    with Dataset(forcing_files[0], \"r\") as sample_nc:\n",
    "        forcing_vars = [var for var in sample_nc.variables if var not in [\"time\", \"Number_of_points\"]]\n",
    "    \n",
    "    # Read time and variable data from all files\n",
    "    merged_data = {var: [] for var in forcing_vars}\n",
    "    merged_time = []\n",
    "    \n",
    "    for file in forcing_files:\n",
    "        with Dataset(file, \"r\") as nc:\n",
    "            merged_time.extend(nc.variables[\"time\"][:])\n",
    "            for var in forcing_vars:\n",
    "                merged_data[var].append(nc.variables[var][:])\n",
    "    \n",
    "    # Concatenate data along the time dimension\n",
    "    for var in forcing_vars:\n",
    "        merged_data[var] = np.concatenate(merged_data[var], axis=0)\n",
    "    \n",
    "    # Write merged data to new NetCDF file\n",
    "    with Dataset(merged_filepath, \"w\", format=\"NETCDF4\") as nc:\n",
    "        nc.createDimension(\"time\", len(merged_time))\n",
    "        nc.createDimension(\"Number_of_points\", 1)\n",
    "        \n",
    "        time_var = nc.createVariable(\"time\", \"f8\", (\"time\",))\n",
    "        time_var.units = \"seconds since 2014-01-01 01:00:00\"\n",
    "        time_var[:] = merged_time\n",
    "        \n",
    "        for var in forcing_vars:\n",
    "            nc_var = nc.createVariable(var, \"f4\", (\"time\", \"Number_of_points\"))\n",
    "            nc_var[:] = merged_data[var]\n",
    "    \n",
    "    print(f\"Merged NetCDF file created: {merged_filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2589587",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### OSVAS #####################################################\n",
    "###### ( OFFLINE SURFEX VALIDATION SYSTEM)########################\n",
    "#### STEP 3: STATION METADATA AND CONFIGURATION OF ###############\n",
    "#### THE FORCING GENERATION ######################################\n",
    "\n",
    "#3.1 Define here what will be the home path for saving SURFEX forcing\n",
    "#from the different stations\n",
    "PROJECTDIR='OSVAS'\n",
    "home = os.path.join(str(Path.home()),PROJECTDIR)  \n",
    "station_name='Majadas_south_test' # A directory with this name will be created in 'home'\n",
    "write_forcing_wup='no' #Set to yes for writing forcing\n",
    "\n",
    "#3.2 Site/experiment parameters needed for Params_config.txt\n",
    "#A series of if statements are used below so that one can generate\n",
    "#forcings for different stations and periods.\n",
    "\n",
    "if station_name=='Majadas_south_test':\n",
    " data_pid='https://meta.icos-cp.eu/objects/dDlpnhS3XKyZjB22MUzP_nAm'\n",
    " #Product ID (PID) of the ICOS object from where to extract forcing variables\n",
    " delta_t=1800                     # Interval in seconds between observations\n",
    " lon=-5.774722                    # Station longitude\n",
    " lat= 39.940556                   # Station latitude\n",
    " alt=258                          # Station altitude\n",
    " height_T=2                       # Height of the temperature measurement\n",
    " height_V=10                      # Height of the windspeed   measurement\n",
    " run_start='2019-2-2 00:00:00'    # Timestamp for the forcing start\n",
    " run_end='2019-3-1 00:00:00'      # Timestamp for the forcing end\n",
    " forcing_format='netcdf'          # Choose between netcdf or ascii\n",
    "\n",
    "if station_name=='Fyodorovskoye_test':\n",
    " data_pid='https://meta.icos-cp.eu/objects/p8vfuGtKaPH90vW6WhKx3q9N'\n",
    " #ICOSCp object from where to extract forcing variables\n",
    " delta_t=1800\n",
    " lon=-5.774722\n",
    " lat= 39.940556\n",
    " alt=258\n",
    " height_T=2\n",
    " height_V=10\n",
    " run_start='2018-1-1 00:00:00'\n",
    " run_end='2019-1-1 00:00:00'\n",
    " forcing_format='ascii'          # Choose between netcdf or ascii\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13989dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### OSVAS #####################################################\n",
    "###### ( OFFLINE SURFEX VALIDATION SYSTEM)########################\n",
    "#### STEP 4: LOAD STATION DATA, ###############\n",
    "#### THE FORCING GENERATION ######################################\n",
    "\n",
    "\n",
    "# The example below shows how to process \"Fluxnet\" type data for \"Majadas del Tietar \" (Spain)\n",
    "# or Fyodorovskoye (Rusia) Other Fluxnet data collections can be browsed here: \n",
    "# https://data.icos-cp.eu/portal/ (Write \"Fluxnet product\" in the field \"Data type\")\n",
    "# For other types of data from ICOS network (i.e. ecosystem stations, oceanic...) \n",
    "# Some more extra work will be needed to adapt this notebook,\n",
    "# because column names, etc... are not homogeneous\n",
    "\n",
    "\n",
    "#4.1: Load Object containing variables, get only the table\n",
    "#     containing the data (as a Pandas dataframe)\n",
    "df=Dobj(data_pid).data\n",
    "# Fluxnet data in ICOS have columns named timestamp and timestamp_end\n",
    "# defining the start and end period assigned to the measurement.\n",
    "# In the next lines the timestamp is substituted by the midpoint between \n",
    "# the start and the end of the measurement\n",
    "df.rename(columns={'TIMESTAMP': 'TIMESTAMP_START'},inplace=True)\n",
    "# Create 'average_timestamp' as the midpoint between 'timestamp_start' and 'timestamp_end'\n",
    "df['TIMESTAMP'] = df['TIMESTAMP_START'] + pd.Timedelta(minutes=30)\n",
    "df.set_index('TIMESTAMP',inplace=True)\n",
    "# Rename the dataframe\n",
    "Station_forcing = df\n",
    "\n",
    "#4.2 Get station data\n",
    "# Use a dictionary to rename original variable names to the ones\n",
    "# used by SURFEX in ASCII filenames or netcdf variable names.\n",
    "# Info about the forcing variables and format in ascii available here:\n",
    "# https://www.umr-cnrm.fr/surfex/spip.php?article214\n",
    "# Also, apply some transformations of data with another dictionary\n",
    "\n",
    "# Define renaming dictionary\n",
    "rename_dict = {\n",
    "    \"TA_F\": \"Forc_TA\",\n",
    "    \"PA_F\": \"Forc_PS\",\n",
    "    \"WS_F\": \"Forc_WIND\",\n",
    "    \"LE_F_MDS\": \"LE\",\n",
    "    \"H_F_MDS\": \"H\"\n",
    "}\n",
    "\n",
    "# Define transformations dictionary\n",
    "transform_dict = {\n",
    "    \"Forc_LW\": lambda df: df[\"LW_IN_F\"],\n",
    "    \"Forc_SNOW\": lambda df: df[\"P_F\"] * 0,\n",
    "    \"Forc_CO2\": lambda df: 0.00062,\n",
    "    \"Forc_DIR_SW\": lambda df: np.where(df[\"SW_IN_F\"] < 0, 0, df[\"SW_IN_F\"]),\n",
    "    \"Forc_SCA_SW\": lambda df: df[\"Forc_DIR_SW\"] * 0.0,  # Initialized as zero\n",
    "    \"Forc_RAIN\": lambda df: df[\"P_F\"] / delta_t,\n",
    "    \"Forc_PS\": lambda df: df[\"Forc_PS\"] * 1000,  # Convert to Pa\n",
    "    \"Forc_DIR\": lambda df: df[\"Forc_WIND\"] * 0,  # No wind direction, use 0\n",
    "    \"Forc_TA\": lambda df: df[\"Forc_TA\"] + 273.15,  # Convert to Kelvin\n",
    "    \"ESAT\": lambda df: meteo.humidity.esat(df[\"Forc_TA\"]),\n",
    "    \"E\": lambda df: df[\"ESAT\"] - df[\"VPD_F\"] * 100,\n",
    "    \"Forc_QA\": lambda df: rh2ah(df[\"E\"] / df[\"ESAT\"], df[\"Forc_PS\"], df[\"Forc_TA\"])\n",
    "}\n",
    "\n",
    "# Apply renaming\n",
    "Station_forcing.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "# Apply transformations\n",
    "for col, func in transform_dict.items():\n",
    "    Station_forcing[col] = func(Station_forcing)\n",
    "\n",
    "# Ensure Forc_SCA_SW is explicitly zero\n",
    "Station_forcing.Forc_SCA_SW.values[:]=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d932f7d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###### OSVAS ####################################################\n",
    "###### ( OFFLINE SURFEX VALIDATION SYSTEM)#######################\n",
    "#### STEP 5: PLOT FORCING VARIABLES FOR THE SELECTED PERIOD #####\n",
    "#### WRITE THE FORCING FILES IN THE SELECTED FILE TYPE ##########\n",
    "\n",
    "\n",
    "Forcing_vars=['Forc_CO2','Forc_DIR','Forc_PS','Forc_RAIN','Forc_SNOW','Forc_WIND','Forc_DIR_SW','Forc_LW','Forc_QA','Forc_SCA_SW','Forc_TA']\n",
    "\n",
    "\n",
    "if forcing_format=='ascii':\n",
    "    write_forcing_ascii(\n",
    "    Forcing_vars=Forcing_vars, \n",
    "    Forcing_path=home + '/' + station_name + '_run/' + 'forcing_run',\n",
    "    Station_forcing=Station_forcing, \n",
    "    run_start=run_start, \n",
    "    run_end=run_end, \n",
    "    delta_t=delta_t, \n",
    "    lon=lon, lat=lat, alt=alt, \n",
    "    height_T=height_T, height_V=height_V,\n",
    "    write_forcing_run='yes'\n",
    ")\n",
    "    \n",
    "if forcing_format=='netcdf':\n",
    "    write_forcing_netcdf(\n",
    "    Forcing_vars=Forcing_vars, \n",
    "    Forcing_path=home + '/' + station_name + '_run/' + 'forcing_run',\n",
    "    Station_forcing=Station_forcing, \n",
    "    run_start=run_start, \n",
    "    run_end=run_end, \n",
    "    delta_t=delta_t, \n",
    "    lon=lon, lat=lat, alt=alt, \n",
    "    height_T=height_T, height_V=height_V,\n",
    "    write_forcing_run='yes'\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "438af085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged NetCDF file created: /home/pn56/OSVAS/Majadas_south_test_run/forcing_run/FORCING.nc\n"
     ]
    }
   ],
   "source": [
    "#### Example on how to reconstruct a forcing file for a period from the individual netcdf files\n",
    "\n",
    "merge_forcing_netcdf(Forcing_path=home + '/' + station_name + '_run/' + 'forcing_run',\n",
    "                     start_date=run_start, end_date=run_end, output_filename=\"FORCING.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43cedbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb820b00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7666c2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e43c295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8de1e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8fdb25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cb2a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0336dde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703b30ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05fba79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9310081d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7d9525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec6cffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a1ea64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff57e856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2602abfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9d9c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24c00f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Station_forcing_run['Forc_DIR'][1:100].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd864b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "Station_forcing_run['Forc_WIND'][1:100].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d638d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Station_forcing_run['Forc_TA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f3c11e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
